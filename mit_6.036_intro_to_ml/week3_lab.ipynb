{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8abcf1d-d671-49cc-8139-7e1100c49680",
   "metadata": {},
   "source": [
    "> For each feature from the following:\n",
    "> [ cylinders, displacement, horsepower, weight, acceleration, model_year, origin]\n",
    "indicate how you can represent it so as to make classification easier and get good generalization on unseen data, by choosing one of:\n",
    "> 'drop' - leave the feature out,\n",
    "> 'raw' - use values as they are,\n",
    "> 'standard' - standardize values by subtracting out average value and dividing by standard deviation,\n",
    "> 'one-hot' - use a one-hot encoding.\n",
    "> There could be multiple answers that make sense for each feature; please mention the tradeoffs between each answer. Write down your choices.\n",
    "\n",
    "\n",
    "- cylinders\n",
    "\n",
    "Raw as value is meaningful, standard\n",
    "\n",
    "- displacement\n",
    "\n",
    "Standard as larger variance.\n",
    "\n",
    "- Horsepower\n",
    "Standard as larger variance.\n",
    "\n",
    "- Weight\n",
    "Standard as larger variance.\n",
    "\n",
    "- Acceleration\n",
    "Raw, or standard\n",
    "\n",
    "- model_year\n",
    "Perhaps change to num of years till 2022. Use raw or standard.\n",
    "\n",
    "- origin\n",
    "One-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda219b8-aa9e-4b08-9e59-af1fe4485be5",
   "metadata": {},
   "source": [
    "> 1C) How can car name, a textual feature, be transformed into a feature which can be used by the perceptron algorithm?\n",
    "\n",
    "1) Tokenize all words in names. And use 'one-hot' that allows several selected.\n",
    "2) Factor names: extract producer name. That is take the first word. Use 'one-hot'\n",
    "3) Use unsupervised learning to get clusters of names\n",
    "4) Order, numerate, standardize.\n",
    "\n",
    "> 1D)\n",
    "> - For each feature representation, compute a classifier by running the algorithm on all the data and compare the number of errors of that classifier on the data.\n",
    "\n",
    "Not good as classifier will fit to the training data.\n",
    "\n",
    "> Split the data into two sets: training is 90%, test is 10%. For each feature representation, compute a classifier by running the algorithm on the training data, compare the sum of the number of errors of that classifier on the training and test data.\n",
    "\n",
    "Not good as comparing the sum so it is similar to the above.\n",
    "\n",
    "> Split the data into two sets: training is 90%, test is 10%. For each feature representation, compute a classifier by running the algorithm on the training data, compare the number of errors of that classifier on the test data.\n",
    "\n",
    "Good.\n",
    "\n",
    "> For each feature representation, perform 10-fold cross-validation and compare the resulting average error rate.\n",
    "\n",
    "Good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15da391-4a29-4a81-a7eb-e34805b83006",
   "metadata": {},
   "source": [
    "2) \n",
    "\n",
    "> 2C) Talk with your partner about the weaknesses of the bag-of-words approach seen above. For instance, how would you interpret the feature vector (1,1,1,1,1,0,1,0)(1,1,1,1,1,0,1,0)(1,1,1,1,1,0,1,0)? (Who is selling what to whom?)\n",
    "\n",
    "Weaknesses:\n",
    "- Sentence units (subject, predicate, objective, etc.) lost, i.e. meaning is lost.\n",
    "- Doesn't count frequency. So 'one plus one plus one plus one' is the same as 'one plus one' but 4 is not 2.\n",
    "- Takes less important words like adjectives, conjuctions, etc.\n",
    "\n",
    "> 2D) Words like \"the\", \"to\", \"a\", \"and\" and so on occur with very high frequency in English sentences. Do they help in detecting the sentiment in a review? \n",
    "\n",
    "Remove them manually. Or perhaps use some threashold: remove all top ones till they some large difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d45cb-66d6-4215-a6b2-19913cfc9fac",
   "metadata": {},
   "source": [
    "3)\n",
    "\n",
    "> With the help of your partner, write down the matrix corresponding to the image shown here:\n",
    "\n",
    "```\n",
    "11011\n",
    "10011\n",
    "11011\n",
    "11011\n",
    "10001\n",
    "\n",
    "11011 10011 11011 11011 10001\n",
    "```\n",
    "\n",
    "> 3C) How well would you expect the perceptron algorithm to work on classifying images if you simply represented them as vectors? Is there any information lost when representing images this way? \n",
    "\n",
    "Might not be linearly separable.\n",
    "Losing dimension info might result in bad predictions?\n",
    "Example is adding 11 at beginning and removing 01 at end:\n",
    "```\n",
    "11110\n",
    "11100\n",
    "11110\n",
    "11110\n",
    "11100\n",
    "```\n",
    "Seems ok? It should be classified '1'. Will perceptron classify such digits?\n",
    "\n",
    "\n",
    ">  3D) What approaches might you suggest to extract more meaningful features from the images, such that the perceptron algorithm might do a good job of classification? (Hint: What makes the image of the digit '1' different compared to the image of the digit '3'?)\n",
    "\n",
    "2 dimentions change, not 1? \n",
    "More dots? Larger, longer?\n",
    "Number of 'ends'?  3 for 3, 2 for 1. \n",
    "\n",
    "> 4)\n",
    "> 4A) How well would you expect the perceptron algorithm to work, if it was given these raw sound clips as input? \n",
    "\n",
    "If it might be not lin. separable then add extra dim. Then how?\n",
    "\n",
    "- Long to converge? What if we have very long and very large amplitude: 0.00000001 and 1? Then it will increment very slowly. So to standardize the data? So that those far will be closer?\n",
    "- What if the sound is shifted towards beginning or end? But we can shift the sample to solve it in the case when we take an 1 sec interval from a larger sound clip.\n",
    "\n",
    "> 4B) What makes a cat's meow different from a dog's bark? If you had a black box which can take in a sound clip and return the 555 most dominant frequencies heard in the sound clip, how would you use the black box and extract features from the sound clips? How would you represent those features as a vector which will then be fed to the perceptron algorithm?\n",
    "\n",
    "A meow is longer than a dog's bark. Also the bark with higher max amplitude.\n",
    "Use that black box to get those 5 frequencies. Then add 'height' feature (diff between min and max) in absolute value, median feature in absolute value, the top frequent one.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
