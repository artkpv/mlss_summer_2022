{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIaEwCD406A"
   },
   "source": [
    "#MIT 6.036 Spring 2019: Homework 3#\n",
    "\n",
    "This colab notebook provides code and a framework for problems 1-7 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week3/week3_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
    "\n",
    "## <section>**Setup**</section>\n",
    "\n",
    "First, download the code distribution for this homework that contains test cases and helper functions.\n",
    "\n",
    "Run the next code block to download and import the code for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "id": "vg6IEo8_lmhL",
    "outputId": "82cde602-44d6-4762-96ff-e0a18f24af89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/art/mydir/ref/Учеба_конспекты_решения/mit_6.036_intro_to_ml\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2YM-_zLf9Bp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing code_for_hw03\n",
      "Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\n",
      "Datasets: super_simple_separable_through_origin(), super_simple_separable(), xor(), xor_more()\n",
      "Tests for part 2: test_linear_classifier_with_features, mul, make_polynomial_feature_fun, \n",
      "                  test_with_features\n",
      "Also loaded: perceptron, one_hot_internal, test_one_hot\n",
      "Importing code_for_hw03 (part 2, imported as hw3)\n",
      "Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\n",
      "         xval_learning_alg, eval_classifier\n",
      "Tests: test_linear_classifier\n",
      "Dataset tools: load_auto_data, std_vals, standard, raw, one_hot, auto_data_and_labels\n",
      "               load_review_data, clean, extract_words, bag_of_words, extract_bow_feature_vectors\n",
      "               load_mnist_data, load_mnist_single\n"
     ]
    }
   ],
   "source": [
    "#!rm -rf code_and_data_for_hw3*\n",
    "#!rm -rf mnist\n",
    "#!wget --quiet https://introml_oll.odl.mit.edu/6.036/static/homework/hw03/code_and_data_for_hw3.zip\n",
    "#!unzip code_and_data_for_hw3.zip\n",
    "#!mv code_and_data_for_hw3/* .\n",
    "  \n",
    "from code_for_hw3_part1 import *\n",
    "import code_for_hw3_part2 as hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2z1zuhqltjBy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tidy_plot in module code_for_hw3_part1:\n",
      "\n",
      "tidy_plot(xmin, xmax, ymin, ymax, center=False, title=None, xlabel=None, ylabel=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tidy_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFxhrJ5XDlvb"
   },
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bhI4dQB1-UZ"
   },
   "source": [
    "## <section>**Running Perceptron**</section>\n",
    "\n",
    "In problems 1,2 and 3, you will have to run the Perceptron algorithm several times to obtain linear classifiers.\n",
    "We provide you with an implementation of the algorithm which you can use to obtain your results.\n",
    "\n",
    "The specifications for the `perceptron`method provided are:\n",
    "* `data` is a numpy array of dimension $d$ by $n$\n",
    "* `labels` is numpy array of dimension $1$ by $n$\n",
    "* `params` is a dictionary specifying extra parameters to this algorithm; your algorithm runs a number of iterations equal to $T$\n",
    "* `hook` is either None or a function that takes the tuple `(th, th0)` as an argument and displays the separator graphically. \n",
    "\n",
    "It should return a tuple of $\\theta$ (a $d$ by 1 array) and $\\theta_0$ (a 1 by 1 array).\n",
    "\n",
    "Note that you are free to modify the method. For example, a useful modification for this homework would be to make the method return the number of mistakes made on the input data, while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VtYf8ysk-VQU"
   },
   "outputs": [],
   "source": [
    "# Perceptron algorithm with offset.\n",
    "# data is dimension d by n\n",
    "# labels is dimension 1 by n\n",
    "# T is a positive integer number of steps to run\n",
    "def perceptron(data, labels, params = {}, hook = None):\n",
    "    # if T not in params, default to 100\n",
    "    T = params.get('T', 50)\n",
    "    (d, n) = data.shape\n",
    "\n",
    "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "    return theta, theta_0\n",
    "\n",
    "def averaged_perceptron(data, labels, params = {}, hook = None):\n",
    "    T = params.get('T', 50)\n",
    "    (d, n) = data.shape\n",
    "\n",
    "    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n",
    "    theta_sum = theta.copy()\n",
    "    theta_0_sum = theta_0.copy()\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            x = data[:,i:i+1]\n",
    "            y = labels[:,i:i+1]\n",
    "            if y * positive(x, theta, theta_0) <= 0.0:\n",
    "                theta = theta + y * x\n",
    "                theta_0 = theta_0 + y\n",
    "                if hook: hook((theta, theta_0))\n",
    "            theta_sum = theta_sum + theta\n",
    "            theta_0_sum = theta_0_sum + theta_0\n",
    "    theta_avg = theta_sum / (T*n)\n",
    "    theta_0_avg = theta_0_sum / (T*n)\n",
    "    if hook: hook((theta_avg, theta_0_avg))\n",
    "    return theta_avg, theta_0_avg\n",
    "\n",
    "  \n",
    "def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n",
    "    th, th0 = learner(data_train, labels_train)\n",
    "    return score(data_test, labels_test, th, th0)/data_test.shape[1]\n",
    "\n",
    "def positive(x, th, th0):\n",
    "    return np.sign(th.T@x + th0)\n",
    "\n",
    "def score(data, labels, th, th0):\n",
    "    return np.sum(positive(data, th, th0) == labels)\n",
    "\n",
    "def xval_learning_alg(learner, data, labels, k):\n",
    "    _, n = data.shape\n",
    "    idx = list(range(n))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(idx)\n",
    "    data, labels = data[:,idx], labels[:,idx]\n",
    "\n",
    "    score_sum = 0\n",
    "    s_data = np.array_split(data, k, axis=1)\n",
    "    s_labels = np.array_split(labels, k, axis=1)\n",
    "    for i in range(k):\n",
    "        data_train = np.concatenate(s_data[:i] + s_data[i+1:], axis=1)\n",
    "        labels_train = np.concatenate(s_labels[:i] + s_labels[i+1:], axis=1)\n",
    "        data_test = np.array(s_data[i])\n",
    "        labels_test = np.array(s_labels[i])\n",
    "        score_sum += eval_classifier(learner, data_train, labels_train,\n",
    "                                              data_test, labels_test)\n",
    "    return score_sum/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g96gm5hmQja"
   },
   "outputs": [],
   "source": [
    "perceptron(data, labels, params = {'T':100}, hook = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_perceptron(data, labels, params={}, hook=None):\n",
    "    # if T not in params, default to 100\n",
    "    T = params.get('T', 100)\n",
    "    d = data.shape[0]\n",
    "    n = data.shape[1]\n",
    "    theta = np.transpose([[0.0] * d])\n",
    "    theta_0 = 0.0\n",
    "    #ax = plot_data(data, labels)\n",
    "    num = 0\n",
    "    for test in range(T):\n",
    "        founderror = False\n",
    "        for i in range(n):\n",
    "            xi = data[:,i:i+1]\n",
    "            yi = labels[0, i]\n",
    "            if yi * np.sign(theta.T @ xi + theta_0) <= 0:\n",
    "                num += 1\n",
    "                founderror = True\n",
    "                theta += yi * xi\n",
    "                theta_0 += yi\n",
    "                if hook:\n",
    "                    hook(theta, theta_0)\n",
    "        if not founderror:\n",
    "            break\n",
    "    #plot_separator(ax=ax, th=theta, th_0=theta_0)\n",
    "    return (theta, np.array([[theta_0]]), num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800.0010249993434, 0.2683281572999748, 8888911.666666666)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [200, 800, 200, 800],\n",
    "    [0.2,  0.2,  0.8,  0.8],\n",
    "    [1, 1, 1, 1]\n",
    "])\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "th = np.array([[0, 1, -0.5]])\n",
    "\n",
    "gamma = np.abs(np.min(labels.T * (th @ data) / np.linalg.norm(th) ))\n",
    "r = np.max(np.linalg.norm(data, axis=0)) \n",
    "r, gamma, (r / gamma)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [200, 800, 200, 800],\n",
    "    [0.2,  0.2,  0.8,  0.8]\n",
    "])\n",
    "labels = np.array([[-1, -1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.],\n",
       "        [600.]]),\n",
       " array([[0.]]),\n",
       " 2000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron(data, labels, params={'T':1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0.],\n",
       "        [6000.]]),\n",
       " array([[0.]]),\n",
       " 20000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "perceptron(data, labels, params={'T':10000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  600.        ],\n",
       "        [69997.80000031]]),\n",
       " array([[0.]]),\n",
       " 233326)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "perceptron(data, labels, params={'T':100000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.000000e+02],\n",
       "        [ 2.000068e+05]]),\n",
       " array([[-4.]]),\n",
       " 666696)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron(data, labels, params={'T':1000000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2806250973645645, 0.00029999996250000706, 18222233.88889067)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [200, 800, 200, 800],\n",
    "    [0.2,  0.2,  0.8,  0.8],\n",
    "    [1, 1, 1, 1]\n",
    "])\n",
    "data[0:2] *= 0.001\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "th = np.array([[0, 1, -0.0005]])\n",
    "\n",
    "gamma = np.abs(np.min(labels.T * (th @ data) / np.linalg.norm(th) ))\n",
    "\n",
    "data, gamma\n",
    "r = np.max(np.linalg.norm(data, axis=0)) \n",
    "r, gamma, (r / gamma)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.50996688705415, 0.2683281572999748, 31.666666666666664)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [200, 800, 200, 800],\n",
    "    [0.2,  0.2,  0.8,  0.8],\n",
    "    [1, 1, 1, 1]\n",
    "])\n",
    "data[0:1] *= 0.001\n",
    "labels = np.array([[-1, -1, 1, 1]])\n",
    "th = np.array([[0, 1, -0.5]])\n",
    "\n",
    "gamma = np.abs(np.min(labels.T * (th @ data) / np.linalg.norm(th) ))\n",
    "\n",
    "data, gamma\n",
    "r = np.max(np.linalg.norm(data, axis=0)) \n",
    "r, gamma, (r / gamma)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.2],\n",
       "        [ 2.8],\n",
       "        [-1. ]]),\n",
       " array([[-1.]]),\n",
       " 11)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron(data, labels, params={'T':1000000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzqUR755Joij"
   },
   "source": [
    "## <section>2D) Encoding Discrete Values</section>\n",
    "\n",
    "It is common to encode sets of discrete values, for machine learning, not as a single multi-valued feature, but using a one hot encoding. So, if there are $k$ values in the discrete set, we would transform that single multi-valued feature into $k$ binary-valued features, in which feature $i$ has value $+1$ if the original feature value was $i$ and has value $0$ (or $-1$) otherwise.\n",
    "\n",
    "Write a function `one_hot` that takes as input $x$, a single feature value (between $1$ and $k$), and $k$, the total possible number of values this feature can take on, and transform it to a numpy column vector of $k$ binary features using a one-hot encoding (remember vectors have zero-based indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.]]), array([[7.]]), array([[3., 1., 1., 3.]]), array([[ 5., -5.]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[2, 3,  4,  5]])\n",
    "labels = np.array([[1, 1, -1, -1]])\n",
    "th, th0 = perceptron(data, labels)\n",
    "\n",
    "th, th0, labels * (th @ data + th0), th @ np.array([[1, 6]]) + th0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((1, 10))\n",
    "a[:, 1] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R11VPNJ3Jpwb"
   },
   "outputs": [],
   "source": [
    "def one_hot(x, k):\n",
    "    a = np.zeros((k, 1))\n",
    "    a[x-1,0] = 1.0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uOdWFevZJs1U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_one_hot(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =   [[2, 3,  4,  5]]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]]),\n",
       " array([[ 1,  1, -1, -1]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =   np.array([[2, 3,  4,  5]])\n",
    "labels = np.array([[1, 1, -1, -1]])\n",
    "data = np.concatenate(\n",
    "    [one_hot(e, 6) for e in data[0]],\n",
    "    axis=1\n",
    ")\n",
    "data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  2.,  1., -2., -1.,  0.]]), array([[0.]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th, th0 = perceptron(data, labels)\n",
    "th.T, th0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[ 0.,  2.,  1., -2., -1.,  0.]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsung = one_hot(1, 6)\n",
    "nokia = one_hot(6, 6)\n",
    "samsung, nokia, th.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.]]), array([[0.]])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ th.T @ xi + th0 for xi in (samsung, nokia)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.]]), array([[0.]])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ (th.T @ xi + th0) / np.linalg.norm(th) for xi in (samsung, nokia)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]]),\n",
       " array([[ 1,  1, -1, -1,  1,  1]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =   [[1, 2, 3, 4, 5, 6]]\n",
    "labels = np.array([[1, 1, -1, -1, 1, 1]])\n",
    "\n",
    "data = np.concatenate(\n",
    "    [one_hot(e, 6) for e in data[0]],\n",
    "    axis=1\n",
    ")\n",
    "data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  1., -2., -2.,  1.,  1.]]), array([[0.]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "th, th0 = perceptron(data, labels)\n",
    "th.T, th0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHCfY1dBJvK3"
   },
   "source": [
    "## 3) Polynomial Features\n",
    "\n",
    "One systematic way of generating non-linear transformations of your input features is to consider the polynomials of increasing order.  Given a feature vector $x = [x_1, x_2, ..., x_d]^T$, we can map it into a new feature vector that contains all the factors in a polynomial of order $d$. For example, for $x = [x_1, x_2]^T$ and order 2, we get $$\\phi(x) = [1, x_1, x_2, x_1x_2, x_1^2, x_2^2]^T$$ and for order 3, we get $$\\phi(x) = [1, x_1, x_2, x_1x_2, x_1^2, x_2^2, x_1^2x_2, x_1x_2^2, x_1^3, x_2^3]^T.$$  \n",
    "\n",
    "In the code that has been loaded, we have defined `make_polynomial_feature_fun` that, given the order, returns a feature transformation function (analogous to $\\phi$ in the description).  You should use it in doing this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "2MHF3Ej7Jx0r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "## For example, make_polynomial_feature_fun could be used as follows:\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "data = np.zeros((5,1))\n",
    "\n",
    "# Generate transformation of order 2\n",
    "transformation = make_polynomial_feature_fun(2)\n",
    "\n",
    "# Use transformation on data\n",
    "print(transformation(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "vro86Am2mPMF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 231, 496, 861, 1326]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Enter a list of 6 integers indicating the number of polynomial features of degrees [1, 10, 20, 30, 40, 50] for a 2-dimensional feature vector.\n",
    "# (x1, x2)\n",
    "# 1: 2\n",
    "# 10:\n",
    "# 1, 1 \n",
    "# 2 11 2\n",
    "# 3 21 12 3\n",
    "# 4 31 22 13 4\n",
    "# 5 41 32 23 14 5\n",
    "# 6 51 42 33 24 15 6\n",
    "# ..\n",
    "# 10 91 82 73 64 55 46 37 28 19 10\n",
    "#     1 + 2 + 3 + 4 + ..\n",
    "# 20: 2 + .. + 21 = 23 * 20 / 2 = 230 \n",
    "# 30: 2 + .. + 31 = 33 * 30 / 2 = 330 + 165 = 495\n",
    "# 40: 43 * 40 / 2 = 860\n",
    "# 50: 53 * 50 / 2 = 5300 / 4 = 1325\n",
    "# 2, 65, 230, 495, 860, 1325 ; +1\n",
    "[1 + (n+3) * n // 2 for n in (10, 20, 30, 40, 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwVEBQ_dMoxM"
   },
   "source": [
    "Note that iterative animations, which update a plot within a loop, don't work the same way in colab, as with a local python console installation.  One workaround for colab to be able to show such plot iterations is to show all the plots, and this can be done for the test code using this patched function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "U87UfhraMn7P"
   },
   "outputs": [],
   "source": [
    "def test_linear_classifier_with_features(dataFun, learner, feature_fun,\n",
    "                             draw = True, refresh = True, pause = True):\n",
    "    raw_data, labels = dataFun()\n",
    "    data = feature_fun(raw_data) if feature_fun else raw_data\n",
    "    if draw:\n",
    "        def hook(params):\n",
    "            ax = plot_data(raw_data, labels)   # create plot axis on each iteration\n",
    "            (th, th0) = params\n",
    "            predictor = lambda x1,x2: int(positive(feature_fun(cv([x1, x2])), th, th0))\n",
    "            plot_nonlin_sep(\n",
    "                predictor,\n",
    "                ax = ax)\n",
    "            plot_data(raw_data, labels, ax)\n",
    "            plt.show()                         # force plot to push to the colab notebook and be displayed\n",
    "            print('th', th.T, 'th0', th0)\n",
    "            if pause: input('press enter here to continue:')\n",
    "    else:\n",
    "        hook = None\n",
    "    th, th0 = learner(data, labels, hook = hook)\n",
    "    if hook: hook((th, th0))\n",
    "    print(\"Final score\", int(score(data, labels, th, th0)))\n",
    "    print(\"Params\", np.transpose(th), th0)\n",
    "\n",
    "def test_with_features(dataFun, order = 2, draw=True, pause=True, learner=perceptron):\n",
    "    test_linear_classifier_with_features(\n",
    "        dataFun,                        # data\n",
    "        learner,                     # learner\n",
    "        make_polynomial_feature_fun(order), # feature maker\n",
    "        draw=draw,\n",
    "        pause=pause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7SLAtCuNSOq"
   },
   "source": [
    "Here's a test you can run to see plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perceptron_with_params(T=100):\n",
    "    myparams ={'T': T}\n",
    "    def f(data, labels, params = {}, hook = None):\n",
    "        return perceptron(data, labels, myparams, hook)\n",
    "    return f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "H74UrlZSNVe-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "Final score 4\n",
      "Params [[  2.   4.  17. -46.  59. 107.]] [[2.]]\n"
     ]
    }
   ],
   "source": [
    "print(super_simple_separable_through_origin()[0].shape)\n",
    "test_with_features(super_simple_separable_through_origin, order=2, draw=False, pause=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "Final score 4\n",
      "Params [[ -11.  -26.   11. -190.  140.  235.]] [[-11.]]\n"
     ]
    }
   ],
   "source": [
    "print(super_simple_separable()[0].shape)\n",
    "test_with_features(super_simple_separable, order=2, draw=False, pause=False, learner=perceptron_with_params(T=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "Final score 4\n",
      "Params [[ 1. -1. -1. -5. 11. -5.]] [[1.]]\n"
     ]
    }
   ],
   "source": [
    "print(xor()[0].shape)\n",
    "test_with_features( xor , order=2, draw=False, pause=False, learner=perceptron_with_params(T=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 2, 1, 2, 2, 4, 1, 3],\n",
      "       [1, 2, 2, 1, 3, 1, 3, 3]]), array([[ 1,  1, -1, -1,  1,  1, -1, -1]]))\n",
      "2202\n",
      "Final score 8\n",
      "Params [[ -78.   28.  -39.   72.  248.  -19.   76. -522.  476. -153.]] [[-78.]]\n"
     ]
    }
   ],
   "source": [
    "print(xor_more())\n",
    "test_with_features( xor_more , order=3, draw=False, pause=False, learner=perceptron_with_params(T=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k049Moe2OKBa"
   },
   "source": [
    "We know that a better way to do this exists (eg using [colab plot animations](https://colab.research.google.com/drive/131wXGA8h8d7llSZxZJ6R4e8nz0ih1WPG#scrollTo=5zVG8JcR4CS2)) - if you are willing to contribute some nice code which lets our plotting functions do this, please do share!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcgPe4-XQ8-b"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erNybvgGRXCf"
   },
   "source": [
    "## 4) Evaluating algorithmic and feature choices for AUTO data\n",
    "\n",
    "We now want to build a classifier for the auto data, focusing on the\n",
    "numeric data.  In the code file for this part of the assignment, we have supplied you\n",
    "with the `load_auto_data` function, that can be used to read the\n",
    "relevant .tsv file.  It will return a list of dictionaries, one for each data item.\n",
    "\n",
    "We then have to specify what feature function to use for each column\n",
    "in the data.  The file `hw3_part2_main.py` has an example for constructing\n",
    "the data and label arrays using `raw` feature function for all the columns.\n",
    "Look at the definition of `features` in `hw3_part2_main.py`, this indicates a feature name to\n",
    "use and then a feature function, there are three defined in the\n",
    "`code_for_hw3_part2.py` file (`raw`, `standard` and `one_hot`).  `raw` just uses\n",
    "the original value, `standard` subtracts out the mean value and\n",
    "divides by the standard deviation and `one_hot` does the encoding\n",
    "described in the notes.\n",
    "\n",
    "The function `auto_data_and_labels` will process the dictionaries and\n",
    "return <tt>data, labels</tt> where <tt>data</tt> are arrays of\n",
    "dimension $(d, 392)$, with $d$ the total number of features specified,\n",
    "and <tt>labels</tt> is of dimension $(1, 392)$.  The data in the file\n",
    "is sorted by class, but it will be shuffled when you read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "3jdynxZqQ5Ky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg and std {}\n",
      "entries in one_hot field {}\n",
      "auto data and labels shape (6, 392) (1, 392)\n"
     ]
    }
   ],
   "source": [
    "# Returns a list of dictionaries.  Keys are the column names, including mpg.\n",
    "auto_data_all = hw3.load_auto_data('auto-mpg.tsv')\n",
    "\n",
    "# The choice of feature processing for each feature, mpg is always raw and\n",
    "# does not need to be specified.  Other choices are hw3.standard and hw3.one_hot.\n",
    "# 'name' is not numeric and would need a different encoding.\n",
    "features = [('cylinders', hw3.raw),\n",
    "            ('displacement', hw3.raw),\n",
    "            ('horsepower', hw3.raw),\n",
    "            ('weight', hw3.raw),\n",
    "            ('acceleration', hw3.raw),\n",
    "            ## Drop model_year by default\n",
    "            ## ('model_year', hw3.raw),\n",
    "            ('origin', hw3.raw)]\n",
    "\n",
    "# Construct the standard data and label arrays\n",
    "auto_data, auto_labels = hw3.auto_data_and_labels(auto_data_all, features)\n",
    "print('auto data and labels shape', auto_data.shape, auto_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "yfOrmU1XdFCZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "97\n",
      "95\n",
      "90\n",
      "98\n",
      "94\n",
      "99\n",
      "97\n",
      "92\n",
      "94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6526282051282052"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw3.xval_learning_alg(\n",
    "    lambda data, labels: perceptron(data, labels, {\"T\": 1}),\n",
    "    auto_data,\n",
    "    auto_labels,\n",
    "    10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441025641025641"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw3.xval_learning_alg(\n",
    "    lambda data, labels: averaged_perceptron(data, labels, {\"T\": 1}),\n",
    "    auto_data,\n",
    "    auto_labels,\n",
    "    10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
      "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
      "auto data and labels shape (12, 392) (1, 392)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features2 = [('cylinders', hw3.one_hot),\n",
    "            ('displacement', hw3.standard),\n",
    "            ('horsepower', hw3.standard),\n",
    "            ('weight', hw3.standard),\n",
    "            ('acceleration', hw3.standard),\n",
    "            ## Drop model_year by default\n",
    "            ## ('model_year', hw3.raw),\n",
    "            ('origin', hw3.one_hot)]\n",
    "\n",
    "# Construct the standard data and label arrays\n",
    "auto_data2, auto_labels2 = hw3.auto_data_and_labels(auto_data_all, features2)\n",
    "print('auto data and labels shape', auto_data2.shape, auto_labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7908333333333333, 0.9004487179487182)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hw3.xval_learning_alg(\n",
    "    lambda data, labels: perceptron(data, labels, {\"T\": 1}),\n",
    "    auto_data2,\n",
    "    auto_labels2,\n",
    "    10),  hw3.xval_learning_alg(\n",
    "    lambda data, labels: averaged_perceptron(data, labels, {\"T\": 1}),\n",
    "    auto_data2,\n",
    "    auto_labels2,\n",
    "    10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7423076923076924, 0.8366025641025641)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw3.xval_learning_alg(\n",
    "    lambda data, labels: perceptron(data, labels, {\"T\": 10}),\n",
    "    auto_data,\n",
    "    auto_labels,\n",
    "    10),  hw3.xval_learning_alg(\n",
    "    lambda data, labels: averaged_perceptron(data, labels, {\"T\": 10}),\n",
    "    auto_data,\n",
    "    auto_labels,\n",
    "    10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545\n",
      "540\n",
      "563\n",
      "529\n",
      "546\n",
      "540\n",
      "531\n",
      "495\n",
      "532\n",
      "547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8061538461538461, 0.8979487179487181)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw3.xval_learning_alg(\n",
    "    lambda data, labels: perceptron(data, labels, {\"T\": 10}),\n",
    "    auto_data2,\n",
    "    auto_labels2,\n",
    "    10),  hw3.xval_learning_alg(\n",
    "    lambda data, labels: averaged_perceptron(data, labels, {\"T\": 10}),\n",
    "    auto_data2,\n",
    "    auto_labels2,\n",
    "    10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6909615384615384, 0.8366025641025641)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hw3.xval_learning_alg(\n",
    "    lambda data, labels: perceptron(data, labels, {\"T\": 50}),\n",
    "    auto_data,\n",
    "    auto_labels,\n",
    "    10),  hw3.xval_learning_alg(\n",
    "    lambda data, labels: averaged_perceptron(data, labels, {\"T\": 50}),\n",
    "    auto_data,\n",
    "    auto_labels,\n",
    "    10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8060256410256409, 0.9005128205128207)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw3.xval_learning_alg(\n",
    "    lambda data, labels: perceptron(data, labels, {\"T\": 50}),\n",
    "    auto_data2,\n",
    "    auto_labels2,\n",
    "    10),  hw3.xval_learning_alg(\n",
    "    lambda data, labels: averaged_perceptron(data, labels, {\"T\": 50}),\n",
    "    auto_data2,\n",
    "    auto_labels2,\n",
    "    10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.98173469],\n",
       "        [ 0.34622449],\n",
       "        [ 0.51530612],\n",
       "        [-0.95596939],\n",
       "        [ 2.80469388],\n",
       "        [-1.46206452],\n",
       "        [ 0.27203955],\n",
       "        [-6.55860703],\n",
       "        [ 0.83288456],\n",
       "        [-0.10352041],\n",
       "        [ 1.1647449 ],\n",
       "        [-0.33270408]]),\n",
       " array([[0.72852041]]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "th, th0 = averaged_perceptron(auto_data2, auto_labels2, params={'T':50})\n",
    "th, th0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh-D5bvTbizf"
   },
   "source": [
    "## 5) Evaluating algorithmic and feature choices for review data\n",
    "\n",
    "We have supplied you with the `load_review_data`\n",
    "function, that can be used to read a .tsv file and return the labels\n",
    "and texts. We have also supplied you with the `bag_of_words` function,\n",
    "which takes the raw data and returns a dictionary of unigram\n",
    "words. The resulting dictionary is an input to\n",
    "`extract_bow_feature_vectors` which computes a feature matrix of ones\n",
    "and zeros that can be used as the input for the classification\n",
    "algorithms.  The file `hw3_part2_main.py` has code for constructing\n",
    "the data and label arrays.  Using these arrays and our implementation\n",
    "of the learning algorithms, you will be able to compute $\\theta$ and\n",
    "$\\theta_0$.  You will need to add your (or the one written by staff)\n",
    "implementation of perceptron and averaged perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5gHVvGl2bsps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_bow_data and labels shape (19945, 10000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Returns lists of dictionaries.  Keys are the column names, 'sentiment' and 'text'.\n",
    "# The train data has 10,000 examples\n",
    "review_data = hw3.load_review_data('reviews.tsv')\n",
    "\n",
    "# Lists texts of reviews and list of labels (1 or -1)\n",
    "review_texts, review_label_list = zip(*((sample['text'], sample['sentiment']) for sample in review_data))\n",
    "\n",
    "# The dictionary of all the words for \"bag of words\"\n",
    "dictionary = hw3.bag_of_words(review_texts)\n",
    "\n",
    "# The standard data arrays for the bag of words\n",
    "review_bow_data = hw3.extract_bow_feature_vectors(review_texts, dictionary)\n",
    "review_labels = hw3.rv(review_label_list)\n",
    "print('review_bow_data and labels shape', review_bow_data.shape, review_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wR9S-6DsdDFF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T 1\n",
      "P 0.7672000000000001\n",
      "7.181673765182495\n",
      "AP 0.8120999999999998\n",
      "7.737758159637451\n",
      "T 10\n",
      "P 0.7871\n",
      "31.119680643081665\n",
      "AP 0.8237\n",
      "38.83753275871277\n",
      "T 50\n",
      "P 0.8036\n",
      "130.36860251426697\n",
      "AP 0.8157\n",
      "179.88108468055725\n"
     ]
    }
   ],
   "source": [
    "for T in (1, 10, 50):\n",
    "    print('T', T)\n",
    "    start = time.time()\n",
    "    print('P', xval_learning_alg(\n",
    "        lambda data, labels: perceptron(data, labels, {\"T\": T}),\n",
    "        review_bow_data, review_labels, 10))\n",
    "    print(time.time() - start)\n",
    "    start = time.time()\n",
    "    print('AP', xval_learning_alg(\n",
    "        lambda data, labels: averaged_perceptron(data, labels, {\"T\": T}),\n",
    "        review_bow_data, review_labels, 10))\n",
    "    print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.15984],\n",
       "        [-2.74048],\n",
       "        [-1.23668],\n",
       "        ...,\n",
       "        [ 0.     ],\n",
       "        [-1.2001 ],\n",
       "        [ 0.     ]]),\n",
       " array([[-1.72795]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th, th0 = averaged_perceptron( review_bow_data, review_labels, {\"T\": 10}) \n",
    "th, th0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['worst',\n",
       "  'awful',\n",
       "  'unfortunately',\n",
       "  'horrible',\n",
       "  'stuck',\n",
       "  'changed',\n",
       "  'disappointment',\n",
       "  'bland',\n",
       "  'poor',\n",
       "  'formula'],\n",
       " ['great',\n",
       "  'individually',\n",
       "  'bright',\n",
       "  'yummy',\n",
       "  'skeptical',\n",
       "  'perfect',\n",
       "  'easily',\n",
       "  'satisfied',\n",
       "  'delicious',\n",
       "  'excellent'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysorted = sorted((e, i) for i,e in enumerate(th))\n",
    "rdict = hw3.reverse_dict(dictionary)\n",
    "[rdict[i] for (e, i) in mysorted[:10]], [rdict[i] for (e, i) in mysorted[-10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbqHMbrubt5t"
   },
   "source": [
    "## 6) Evaluating features for MNIST data\n",
    "\n",
    "\n",
    "This problem explores how well the perceptron algorithm works to <a\n",
    "href=\"http://neuralnetworksanddeeplearning.com/chap1.html\">classify\n",
    "images of handwritten digits</a>, from the well-known (\"MNIST\")\n",
    "dataset, buiding on your thoughts from lab about extracting features\n",
    "from images.  This exercise will highlight how important feature\n",
    "extraction is, before linear classification is done, using algorithms\n",
    "such as the perceptron.\n",
    "\n",
    "<b>Dataset setup</b>\n",
    "\n",
    "Often, it may be easier to work with a vector whose spatial orientation is preserved.\n",
    "In previous parts, we have represented features as one long feature vector.\n",
    "For images, however, we often represent a $m$ by $n$ image\n",
    "as a `(m,n)` array, rather than a `(mn,1)` array\n",
    "(as the previous parts have done).\n",
    "\n",
    "In the code file, we have supplied you with the `load_mnist_data` function,\n",
    "which will read from the provided image files and populate a dictionary,\n",
    "with image and label vectors for each numerical digit from 0 to 9.\n",
    "These images are already shaped as `(m,n)` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "_6xT_UA2cJMe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_data_all loaded. shape of single images is (28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist_data_all = hw3.load_mnist_data(range(10))\n",
    "\n",
    "print('mnist_data_all loaded. shape of single images is', mnist_data_all[0][\"images\"][0].shape)\n",
    "\n",
    "# HINT: change the [0] and [1] if you want to access different images\n",
    "def get_data_lables(leftd, rightd):\n",
    "    d0 = mnist_data_all[leftd][\"images\"]\n",
    "    d1 = mnist_data_all[rightd][\"images\"]\n",
    "    y0 = np.repeat(-1, len(d0)).reshape(1,-1)\n",
    "    y1 = np.repeat(1, len(d1)).reshape(1,-1)\n",
    "    # data goes into the feature computation functions\n",
    "    data = np.vstack((d0, d1))\n",
    "    # labels can directly go into the perceptron algorithm\n",
    "    labels = np.vstack((y0.T, y1.T)).T\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [5.],\n",
       "       [6.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.average([[1, 2, 3], [4, 5, 6], [7, 8, 9]], axis=0)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  5.,  6.,  7.],\n",
       "       [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(24).reshape(2,3,4)\n",
    "print(arr.ndim)\n",
    "print(arr)\n",
    "out_ = np.array([\n",
    "    np.apply_along_axis(\n",
    "        lambda a: np.average(a),\n",
    "        axis=0,\n",
    "        arr=item\n",
    "    )\n",
    "    for item\n",
    "    in arr\n",
    "])\n",
    "out_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "vsX_9X7Nb0NW"
   },
   "outputs": [],
   "source": [
    "# change these implementations to support whole datasets\n",
    "\n",
    "def raw_mnist_features(x):\n",
    "    \"\"\"\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (m*n,n_samples) reshaped array where each entry is preserved\n",
    "    \"\"\"\n",
    "    n_samples, m, n = x.shape\n",
    "    return x.reshape((n_samples, n*m, 1))\n",
    "\n",
    "    \n",
    "def row_average_features(x):\n",
    "    \"\"\"\n",
    "    This should either use or modify your code from the tutor questions.\n",
    "\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (m,n_samples) array where each entry is the average of a row\n",
    "    \"\"\"\n",
    "    return np.apply_along_axis(\n",
    "        lambda a: [np.average(a)],\n",
    "        axis=1,\n",
    "        arr=x\n",
    "    )\n",
    "\n",
    "def col_average_features(x):\n",
    "    \"\"\"\n",
    "    This should either use or modify your code from the tutor questions.\n",
    "\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (n,n_samples) array where each entry is the average of a column\n",
    "    \"\"\"\n",
    "    return np.apply_along_axis(\n",
    "        lambda a: [np.average(a)],\n",
    "        axis=0,\n",
    "        arr=x\n",
    "    ).T\n",
    "\n",
    "def top_bottom_features(x):\n",
    "    \"\"\"\n",
    "    This should either use or modify your code from the tutor questions.\n",
    "\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (2,n_samples) array where the first entry of each column is the average of the\n",
    "    top half of the image = rows 0 to floor(m/2) [exclusive]\n",
    "    and the second entry is the average of the bottom half of the image\n",
    "    = rows floor(m/2) [inclusive] to m\n",
    "    \"\"\"\n",
    "    n, m = x.shape\n",
    "    return cv([np.average(x[0:n//2,:]), np.average(a=x[n//2:,:])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0], [4.666666666666667]]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your Code Here\n",
    "ans=row_average_features(np.array([[1,2,3],[3,9,2]])).tolist()\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0], [4.0], [4.666666666666667]]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans=col_average_features(np.array([[1,2,3],[3,9,2],[2,1,9]])).tolist()\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.5],\n",
       "        [7.5]]),\n",
       " array([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_bottom_features(np.arange(12).reshape((3,4))), np.arange(12).reshape((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "9DTXfMoDgCKk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0.46145176887512207\n",
      "left 0\n",
      "right 1\n",
      "0.975\n",
      "Done 0.47538232803344727\n",
      "left 2\n",
      "right 4\n",
      "0.8641666666666665\n",
      "Done 0.4565155506134033\n",
      "left 6\n",
      "right 8\n",
      "0.9479166666666667\n",
      "Done 0.500361442565918\n",
      "left 9\n",
      "right 0\n",
      "0.6470833333333333\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# use this function to evaluate accuracy\n",
    "#print( data.shape, raw_mnist_features(data).shape, raw_mnist_features(data).T[0].shape )\n",
    "\n",
    "for l, r in ((0, 1), (2, 4), (6, 8), (9, 0)):\n",
    "    start = time.time()\n",
    "        \n",
    "    data, labels = get_data_lables(l, r)\n",
    "    raw_data = raw_mnist_features(data).T[0]\n",
    "    acc = hw3.get_classification_accuracy(raw_data, labels)\n",
    "    print('Done', time.time() - start)\n",
    "    print('left', l)\n",
    "    print('right', r)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.975, 0.8641666666666665, 0.9479166666666667, 0.6470833333333333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 28, 28)\n",
      "(28, 160)\n",
      "(28, 160)\n",
      "(2, 160)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58, 160)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = get_data_lables(0, 1)\n",
    "print(data.shape)\n",
    "rfd = np.concatenate(\n",
    "    list(\n",
    "        row_average_features(xi).T\n",
    "        for xi\n",
    "        in data\n",
    "    )\n",
    ").T\n",
    "cfd = np.concatenate(\n",
    "    list(\n",
    "        col_average_features(xi).T\n",
    "        for xi\n",
    "        in data\n",
    "    )\n",
    ").T\n",
    "tbfd = np.concatenate(\n",
    "    list(\n",
    "        top_bottom_features(xi).T\n",
    "        for xi\n",
    "        in data\n",
    "    )\n",
    ").T\n",
    "\n",
    "print(rfd.shape)\n",
    "print(cfd.shape)\n",
    "print(tbfd.shape)\n",
    "    \n",
    "np.concatenate(\n",
    "    (rfd, cfd, tbfd),\n",
    "    axis=0\n",
    ").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48125, 0.6375, 0.48125]\n",
      "[0.7754166666666668, 0.49749999999999994, 0.49749999999999994]\n",
      "[0.92125, 0.52125, 0.5650000000000001]\n",
      "[0.49749999999999994, 0.5041666666666667, 0.49749999999999994]\n"
     ]
    }
   ],
   "source": [
    "for l, r in ((0, 1), (2, 4), (6, 8), (9, 0)):\n",
    "    data, labels = get_data_lables(l, r)\n",
    "    rfd = np.concatenate(\n",
    "        list(\n",
    "            row_average_features(xi).T\n",
    "            for xi\n",
    "            in data\n",
    "        )\n",
    "    ).T\n",
    "    cfd = np.concatenate(\n",
    "        list(\n",
    "            col_average_features(xi).T\n",
    "            for xi\n",
    "            in data\n",
    "        )\n",
    "    ).T\n",
    "    tbfd = np.concatenate(\n",
    "        list(\n",
    "            top_bottom_features(xi).T\n",
    "            for xi\n",
    "            in data\n",
    "        )\n",
    "    ).T\n",
    "    res = []\n",
    "    for fdata in (rfd, cfd, tbfd):\n",
    "        acc = hw3.get_classification_accuracy(fdata, labels)\n",
    "        res += [acc]\n",
    "    print(repr(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.48125,\n",
    "0.6375,\n",
    "0.48125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 6.2F) (Optional) What does it mean if a binary classification accuracy is below 0.5, if your dataset is balanced (same number from each class)? Are these datasets balanced?\n",
    "\n",
    "Means it is worse than randomly picking up labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.48250000000000004, 5, 8), (0.4841666666666667, 4, 9), (0.5079166666666667, 4, 6), (0.525, 1, 8), (0.5429166666666666, 9, 8), (0.5487500000000001, 8, 5), (0.575, 5, 3), (0.5758333333333334, 3, 5), (0.5912499999999999, 7, 3), (0.5954166666666667, 0, 5)]\n",
      "[(0.9737500000000001, 9, 2), (0.975, 0, 1), (0.975, 1, 0), (0.975, 9, 1), (0.9808333333333333, 7, 5), (0.98125, 1, 4), (0.98125, 5, 7), (0.9875, 4, 1), (0.9875, 4, 3), (0.99375, 8, 0)]\n"
     ]
    }
   ],
   "source": [
    "# 6.2G) (Optional) Feel free to classify other images from each other. Which combinations perform the best, and which perform the worst? Do these make sense? Other than row and column average, are there any other features you could think of that would preserve some spatial information? \n",
    "\n",
    "res = []\n",
    "for l in range(10):\n",
    "    for r in range(10):\n",
    "        if l == r:\n",
    "            continue\n",
    "        start = time.time()\n",
    "        data, labels = get_data_lables(l, r)\n",
    "        raw_data = raw_mnist_features(data).T[0]\n",
    "        acc = hw3.get_classification_accuracy(raw_data, labels)\n",
    "        res += [(acc, l, r)]\n",
    "sres = sorted(res)\n",
    "print(sres[:10])\n",
    "print(sres[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 10:\n",
      "8 vs 0: 0.99375\n",
      "4 vs 3: 0.9875\n",
      "4 vs 1: 0.9875\n",
      "5 vs 7: 0.98125\n",
      "1 vs 4: 0.98125\n",
      "7 vs 5: 0.9808333333333333\n",
      "9 vs 1: 0.975\n",
      "1 vs 0: 0.975\n",
      "0 vs 1: 0.975\n",
      "9 vs 2: 0.9737500000000001\n",
      "Worst 10:\n",
      "5 vs 8: 0.48250000000000004\n",
      "4 vs 9: 0.4841666666666667\n",
      "4 vs 6: 0.5079166666666667\n",
      "1 vs 8: 0.525\n",
      "9 vs 8: 0.5429166666666666\n",
      "8 vs 5: 0.5487500000000001\n",
      "5 vs 3: 0.575\n",
      "3 vs 5: 0.5758333333333334\n",
      "7 vs 3: 0.5912499999999999\n",
      "0 vs 5: 0.5954166666666667\n"
     ]
    }
   ],
   "source": [
    "print('Best 10:')\n",
    "print('\\n'.join('{} vs {}: {}'.format(l, r, a) for a,l,r in reversed(sres[-10:])))\n",
    "print('Worst 10:')\n",
    "print('\\n'.join('{} vs {}: {}'.format(l, r, a) for a,l,r in sres[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIT6.036 hw03 colab notebook",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
