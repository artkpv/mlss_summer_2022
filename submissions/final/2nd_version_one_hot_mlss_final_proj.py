# -*- coding: utf-8 -*-
"""Onehot version mlss_final_proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cdi5kPxwoOqbJ45_t61pgqp-KzzS1Hd2

# MLSS final project. Safe MNIST classifier.
"""

!pip install torch torchvision foolbox opendatasets pandas

#Import Libraries


from __future__ import print_function
import argparse
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import torchvision.transforms as T
from torch.autograd import Variable
import matplotlib.pyplot as plt
import numpy as np
from foolbox import PyTorchModel as FBModel, attacks as fbattacks, utils as fbutils, plot as fbplot, criteria as fbcriteria
import pandas as pd
import opendatasets as od
import sklearn.metrics as sk
import os
from collections import defaultdict
from PIL import Image

args={}
if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')        

kwargs={}
args['batch_size_conv']=120
args['test_batch_size_conv']=120
args['batch_size_ood']=1
args['batch_size_outlier'] = 120
args['batch_size']=1000
args['test_batch_size']=1000
args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. 
args['ensemble_epochs'] = 1
args['lr']=0.01 #Learning rate is how fast it will decend.
args['lr_conv']=0.001 #Learning rate is how fast it will decend.
args['weight_decay']=0.999 #L2 penalty
args['gamma']=0.98 #exponential decay of our learning rate
args['momentum']=0.5 
args['num_workers'] = 10
args['seed']=1 #random seed
args['log_interval']=10
args['cuda']= args['device'] == torch.device('cuda')

"""# Load data"""

#download the mnist dataset for training
mnist_train = datasets.MNIST(
    '../data',
    train=True,
    download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        ]),
    target_transform=transforms.Compose([
                        lambda x:torch.LongTensor([x]), 
                        lambda x:F.one_hot(x,10)
                        ])
    )
mnist_train_n = len(mnist_train)
print('mnist_train_n', mnist_train_n)
train_subset, val_subset = torch.utils.data.random_split(
  mnist_train,
  [int(mnist_train_n * 5/6), int(mnist_train_n*1/6)],
  generator=torch.Generator().manual_seed(1)
)

od.download("https://www.kaggle.com/datasets/lubaroli/notmnist")

!tar -xvf /content/notmnist/notMNIST_small.tar.gz

os.remove('./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png')
os.remove('./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png')

notmnist = datasets.ImageFolder('./notMNIST_small/', transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor()]),
    target_transform=transforms.Compose([
        lambda x:torch.LongTensor([x]),
        lambda x:F.one_hot(x,10)
        ])  
    )

notmnist_n = len(notmnist)

val_ood, test_ood = torch.utils.data.random_split(
  notmnist,
  [int(notmnist_n * 1/2), int(notmnist_n * 1/2)],
  generator=torch.Generator().manual_seed(1)
)

ood_val_loader = torch.utils.data.DataLoader(
  val_ood,
  batch_size = args['batch_size_ood'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

ood_test_loader = torch.utils.data.DataLoader(
  test_ood,
  batch_size = args['batch_size_ood'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

fashion_train = datasets.FashionMNIST('./data',
                                     train=True, 
                                     download=True, 
                                     transform=transforms.ToTensor(), 
                                     target_transform=transforms.Compose([
                                        lambda x:torch.LongTensor([x]), # or just torch.tensor
                                        lambda x:F.one_hot(x,10)
                                        ])
                                     )
fashion_test = datasets.FashionMNIST('./data',
                                     train=False, 
                                     download=True, 
                                     transform=transforms.ToTensor(), 
                                     target_transform=transforms.Compose([
                                        lambda x:torch.LongTensor([x]), # or just torch.tensor
                                        lambda x:F.one_hot(x,10)
                                        ])
                                     )
fashion_train_n = len(fashion_train)
fashion_train_subset, fashion_val_subset = torch.utils.data.random_split(
  fashion_train,
  [int(fashion_train_n * 5/6), int(fashion_train_n*1/6)],
  generator=torch.Generator().manual_seed(1)
)
fashion_test = datasets.FashionMNIST('./data', train=False, download=True, transform=transforms.ToTensor())

outlier_loader = torch.utils.data.DataLoader(
  fashion_train_subset,
  batch_size = args['batch_size_outlier'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

outlier_val_loader = torch.utils.data.DataLoader(
  fashion_val_subset,
  batch_size = args['batch_size_ood'], 
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

outlier_test_loader = torch.utils.data.DataLoader(
  fashion_test,
  batch_size = args['batch_size_ood'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

"""# Data Augmentation

Gaussian Noise
"""

def gaussian_noise(train_data, perc_noisy=100):
    num_noisy = len(train_data) * perc_noisy/100
    noisy_train_data = []; 
    idx = np.random.randint(0, len(train_data), int(num_noisy))      
    for i in range(len(train_data)):
      label = train_data[i][1]
      noisy_train_data.append((train_data[i][0],label))
      if i in idx:
        noise_multiplier = torch.rand(1)
        noisy_example = train_data[i][0] + noise_multiplier/2 * torch.randn(28,28)
        noisy_train_data.append((noisy_example,label))
    return noisy_train_data

mnist_train_noise_10 = gaussian_noise(mnist_train, perc_noisy = 10)

mnist_train_noise_15 = gaussian_noise(mnist_train, perc_noisy = 15)

mnist_train_noise_20 = gaussian_noise(mnist_train, perc_noisy = 20)

mnist_train_noise_25 = gaussian_noise(mnist_train, perc_noisy = 25)

mnist_train_noise_30 = gaussian_noise(mnist_train, perc_noisy = 30)

len(mnist_train_noise_10)

for i in range(9):  
  plt.subplot(330 + 1 + i)  
  plt.imshow(mnist_train_noise_10[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_noise_10[i][1])

"""Rotation (and Gaussian Noise)"""

def random_rotation(train_data, perc_rotated=100, randomNoise=False):
    num_rotated = len(train_data) * perc_rotated/100
    rotated_train_data = []; 
    rotater = T.RandomRotation(degrees=(0, 180))
    idx = np.random.randint(0, len(train_data), int(num_rotated))  

    for i in range(len(train_data)):
      label = train_data[i][1]
      rotated_train_data.append((train_data[i][0],label))
      if i in idx:
        rotated_example = rotater(train_data[i][0])
        if randomNoise:
          noise_multiplier = torch.rand(1)
          rotated_example = rotated_example + noise_multiplier/2 * torch.randn(28,28)
          label = train_data[i][1]
        rotated_train_data.append((rotated_example,label))
    return rotated_train_data

mnist_train_rot_10 = random_rotation(mnist_train, perc_rotated = 10, randomNoise=True)

mnist_train_rot_15 = random_rotation(mnist_train, perc_rotated = 15, randomNoise=True)

mnist_train_rot_20 = random_rotation(mnist_train, perc_rotated = 20, randomNoise=True)

mnist_train_rot_25 = random_rotation(mnist_train, perc_rotated = 25, randomNoise=True)

mnist_train_rot_30 = random_rotation(mnist_train, perc_rotated = 30, randomNoise=True)

len(mnist_train_rot_10)

for i in range(9):  
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_rot_10[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_rot_10[i][1])

"""Flipping"""

def flipping(train_data):
  
  p_h = torch.randn(1)
  hflipper = T.RandomHorizontalFlip(p_h)
  p_v = torch.randn(1)
  vflipper = T.RandomVerticalFlip(p_v)

  flipped_train_data = []

  for i in range(len(train_data)):
    flipped_train_data.append((train_data[i][0], train_data[i][1]))
    flipped_1 = hflipper(train_data[i][0])
    flipped_2 = vflipper(flipped_1)
    label = train_data[i][1]
    if torch.equal(flipped_2, train_data[i][0]):
      continue
    else:  
      flipped_train_data.append((flipped_2, label))
  return flipped_train_data

mnist_train_dataug_flip = flipping(mnist_train)

len(mnist_train_dataug_flip)

for i in range(9):  
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_dataug_flip[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_dataug_flip[i][1])

"""Inversion"""

def inversion(train_data, perc_inverted=100):
  
  inverter = T.RandomInvert()
  inverted_train_data = []
  num_inv = len(train_data) * perc_inverted/100
  idx = np.random.randint(0, len(train_data), int(num_inv))  
  for i in range(len(train_data)):
    inverted_train_data.append((train_data[i][0],train_data[i][1]))
    if i in idx:
      inverted_img = inverter(train_data[i][0]) 
      label = train_data[i][1]
      inverted_train_data.append((inverted_img, label))
  return inverted_train_data

mnist_train_inverted = inversion(mnist_train, perc_inverted=10)

len(mnist_train_inverted)

for i in range(9):  
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_inverted[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_inverted[i][1])

"""MixUp"""

def mixup(train_data, perc_mixed=100):
    
    num_mixed = len(train_data) * perc_mixed/100
    #images = []; labels = [];
    mixup_data=[];
    idx_list = np.random.randint(0, len(train_data), int(num_mixed))  
    for idx in range(len(train_data)):
      mixup_data.append((train_data[idx][0], train_data[idx][1]))
      if idx in idx_list:

          image = train_data[idx][0]
          label = train_data[idx][1]
          
          # Choose another image/label randomly
          mixup_idx = np.random.randint(0, len(train_data))
          mixup_image = train_data[mixup_idx][0]
          mixup_label = train_data[mixup_idx][1]
          # Select a random number from the given beta distribution
          # Mixup the images accordingly
          alpha = 0.2
          lam = np.random.beta(alpha, alpha)
          image = lam * image + (1 - lam) * mixup_image
          label = lam * label + (1 - lam) * mixup_label
          
          #images.append(image)
          #labels.append(label)
          mixup_data.append((image,label))
    return mixup_data

mnist_train_mixup_10 = mixup(mnist_train, 10)

mnist_train_mixup_15 = mixup(mnist_train, 15)

mnist_train_mixup_20 = mixup(mnist_train, 20)

mnist_train_mixup_25 = mixup(mnist_train, 25)

mnist_train_mixup_30 = mixup(mnist_train, 30)

for i in range(9):
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_mixup_10[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_mixup_10[i][1])

"""CutMix"""

def rand_bbox(size, lam):
    """ Generate random bounding box 
    Args:
        - size: [width, breadth] of the bounding box
        - lamb: (lambda) cut ratio parameter
    Returns:
        - Bounding box
    """
    W = size[0]
    H = size[1]
    cut_ratio = np.sqrt(1. - lam)
    cut_w = np.int(W * cut_ratio)
    cut_h = np.int(H * cut_ratio)

    # uniform
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2

mnist_train_list = []
for i in range(len(mnist_train)):
  mnist_train_list.append(list(mnist_train[i]))

def generate_cutmix_image(train_data, perc_cutmix=100, beta=1.0):

    cutmix_examples = []
    num_cutmixed = len(train_data) * perc_cutmix/100
    idx_list = np.random.randint(0, len(train_data), int(num_cutmixed))  

    for idx in range(len(train_data)):
      cutmix_examples.append((train_data[idx][0],train_data[idx][1]))
      if idx in idx_list:
        lam = np.random.beta(beta, beta)
        idx_a = idx
        idx_b = np.random.randint(len(train_data))
        img_a = train_data[idx_a][0]
        img_b = train_data[idx_b][0]
        target_a = train_data[idx_a][1]
        target_b = train_data[idx_b][1]
        bbx1, bby1, bbx2, bby2 = rand_bbox(train_data[idx_a][0].squeeze(0).shape, lam)
        #train_data_updated = train_data.copy()
        train_data_updated = train_data.copy()

        img_a_box = img_a[bbx1:bbx2, bby1:bby2]
        img_b_box = img_b[bbx1:bbx2, bby1:bby2]
        img_a[bbx1:bbx2, bby1:bby2] = img_b_box
        img_b[bbx1:bbx2, bby1:bby2] = img_a_box
        train_data_updated[idx_a][0] = img_a
        train_data_updated[idx_b][0] = img_b

      # adjust lambda to exactly match pixel ratio
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (28 * 28))
        label = target_a * lam + target_b * (1. - lam)
        cutmix_examples.append((train_data_updated[idx_a][0],label))
    return cutmix_examples
    #return train_data_updated, labels

# Generate CutMix image
# Let's use the first image of the batch as the input image to be augmented
mnist_train_cutmix_10 = generate_cutmix_image(mnist_train_list, perc_cutmix=10, beta=1.0)

mnist_train_cutmix_15 = generate_cutmix_image(mnist_train_list, perc_cutmix=15, beta=1.0)

mnist_train_cutmix_20 = generate_cutmix_image(mnist_train_list, perc_cutmix=20, beta=1.0)

mnist_train_cutmix_25 = generate_cutmix_image(mnist_train_list, perc_cutmix=25, beta=1.0)

mnist_train_cutmix_30 = generate_cutmix_image(mnist_train_list, perc_cutmix=30, beta=1.0)

# Show CutMix images

for i in range(9):
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_cutmix_10[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_cutmix_10[i][1])

"""# CNN ensemble

In this section we will implement three convolutional neural networks and combine them in an ensemble as in this paper: https://arxiv.org/pdf/2008.10400v2.pdf
"""

# load the data with validation set for conv ensemble

train_loader = torch.utils.data.DataLoader(
  train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

val_loader = torch.utils.data.DataLoader(
  val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data',
                   train=False,
                   transform=transforms.Compose([
                       transforms.ToTensor()
                   ]),
                   target_transform=transforms.Compose([
                        lambda x:torch.LongTensor([x]), # or just torch.tensor
                        lambda x:F.one_hot(x,10)
                        ])),
    batch_size=args['test_batch_size_conv'], shuffle=True, **kwargs)

"""## Architecture"""

#as in the paper, except GELU instead of RELU
class conv3(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 48, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(48),
            nn.GELU(), 

            nn.Conv2d(48, 64, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(),

            nn.Conv2d(64, 80, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(80),
            nn.GELU(),

            nn.Conv2d(80, 96, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(),

            nn.Conv2d(96, 112, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(112),
            nn.GELU(),

            nn.Conv2d(112, 128, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(128),
            nn.GELU(),

            nn.Conv2d(128, 144, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(144),
            nn.GELU(),

            nn.Conv2d(144, 160, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(160),
            nn.GELU(),

            nn.Conv2d(160, 176, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(176),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(11264, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

class conv3_relu(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(32),
            nn.ReLU(), 

            nn.Conv2d(32, 48, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(48),
            nn.ReLU(), 

            nn.Conv2d(48, 64, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.Conv2d(64, 80, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(80),
            nn.ReLU(),

            nn.Conv2d(80, 96, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(96),
            nn.ReLU(),

            nn.Conv2d(96, 112, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(112),
            nn.ReLU(),

            nn.Conv2d(112, 128, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.Conv2d(128, 144, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(144),
            nn.ReLU(),

            nn.Conv2d(144, 160, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(160),
            nn.ReLU(),

            nn.Conv2d(160, 176, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(176),
            nn.ReLU(),

            nn.Flatten(1, -1),
            nn.Linear(11264, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#trains faster, still 99% acc in first epoch!
class conv3_(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 48, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(48),
            nn.GELU(), 

            nn.Conv2d(48, 64, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(30976, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

class conv5_relu(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(32),
            nn.ReLU(), 

            nn.Conv2d(32, 64, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(64),
            nn.ReLU(), 

            nn.Conv2d(64, 96, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(96),
            nn.ReLU(),

            nn.Conv2d(96, 128, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.Conv2d(128, 160, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(160),
            nn.ReLU(),

            nn.Flatten(1, -1),
            nn.Linear(10240, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#as in the paper except for GELU
class conv5(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 64, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(), 

            nn.Conv2d(64, 96, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(),

            nn.Conv2d(96, 128, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(128),
            nn.GELU(),

            nn.Conv2d(128, 160, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(160),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(10240, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#only first three layers from the paper to train faster
class conv5_(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 64, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(), 

            nn.Conv2d(64, 96, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(24576, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#as in the paper except for GELU
class conv7(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 48, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(48),
            nn.GELU(), 

            nn.Conv2d(48, 96, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(), 

            nn.Conv2d(96, 144, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(144),
            nn.GELU(),

            nn.Conv2d(144, 192, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(192),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(3072, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

class conv7_relu(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 48, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(48),
            nn.ReLU(), 

            nn.Conv2d(48, 96, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(96),
            nn.ReLU(), 

            nn.Conv2d(96, 144, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(144),
            nn.ReLU(),

            nn.Conv2d(144, 192, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(192),
            nn.ReLU(),

            nn.Flatten(1, -1),
            nn.Linear(3072, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

"""## Train & test"""

from numpy.lib import shape_base
def test(model, loader):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in loader:
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        with torch.no_grad():
          output = model(data)
          #because we use one hot
          target = torch.squeeze(target, dim = 1)
          target = torch.argmax(target, dim = 1)
          test_loss += F.cross_entropy(output, target, reduction = 'sum').data.item()
          pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
          correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()

    test_loss /= len(loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(val_loader.dataset),
        100. * correct / len(val_loader.dataset)))

def cross_entropy_loss(output, target):
  output = F.softmax(output).log()
  target = target.squeeze(dim = 1)
  loss = torch.mean(torch.sum(-output*target, dim = 1))
  return loss

from IPython.core.inputtransformer2 import TransformerManager
def train(
    train_loader,
    test_loader,
    model,
    num_epochs = 10, 
    batch_size = 120, 
    adversarial_attack=None,
    adversarial_lambda=0.0,
    outlier_loader = None,
    outlier_lambda = 0.5 #in Dan's paper it was 0.5 for vision tasks
  ):
  optimizer = optim.Adam(model.parameters(), lr=args['lr_conv'], weight_decay=args['weight_decay'])
  scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=args['gamma'])

  #outlier exposure with fashion MNIST dataset
  if outlier_loader:
    for i in range(1, num_epochs+1):
      model.train()
      for in_data, out_data in zip(train_loader, outlier_loader):
        data, target = in_data
        target = target.float()
        ood_data, _ = out_data
        if args['cuda']:
          data, target, ood_data = data.cuda(), target.cuda(), ood_data.cuda()
        data, target, ood_data = Variable(data), Variable(target), Variable(ood_data)
        optimizer.zero_grad()
        output = model(data)
        output = output.float()
        adoutput = model(adversarial_attack(data)) if adversarial_attack else None
        loss = cross_entropy_loss(output, target)
        if adversarial_attack:
          loss += adversarial_lambda * cross_entropy_loss(adoutput, target)
        ood_output = model(ood_data)
        loss += outlier_lambda * (-ood_output.mean(dim = 1)+torch.logsumexp(ood_output, dim = 1)).mean()
        loss.backward()
        optimizer.step()
        scheduler.step()
      print('epoch: ', i) 
      test(model, test_loader)     

  else:
    for i in range(1, num_epochs+1):
      model.train()
      for batch_idx, (data, target) in enumerate(train_loader):
        if args['cuda']:
          data, target = data.cuda(), target.cuda()
        #Variables in Pytorch are differenciable. 
        data, target = Variable(data), Variable(target)
        target = target.float()
        #This will zero out the gradients for this batch. 
        optimizer.zero_grad()
        output = model(data)
        output = output.float()
        adoutput = model(adversarial_attack(data, target)) if adversarial_attack else None
        #output = torch.log(output)
        loss = cross_entropy_loss(output, target)
        if adversarial_attack:
          loss += adversarial_lambda * cross_entropy_loss(adoutput, target)
        #dloss/dx for every Variable 
        loss.backward()
        #to do a one-step update on our parameter.
        optimizer.step()
        scheduler.step()
      print('epoch: ', i)
      test(model, test_loader)

model = conv3_() # I am using the model with less layers than the original paper
if args['cuda']:
    model.cuda()

#printing validation loss after first epoch
train(train_loader, val_loader, model, num_epochs = args['ensemble_epochs'])

class Ensemble(nn.Module):
    def __init__(self, models, temp):
        super().__init__()
        self.models = models
        self.temp = temp

    def forward(self, x):
        out = sum(model(x) for model in self.models)
        return out / self.temp

model1oe = conv3()
model2oe = conv5()
model3oe = conv7()
model4oe = conv3_()
model5oe = conv5_()

if args['cuda']:
    model1oe.cuda()
    model2oe.cuda()
    model3oe.cuda()
    model4oe.cuda()
    model5oe.cuda()

train(train_loader, val_loader, model1oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model2oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model3oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model4oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model5oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

model1r = conv3_relu()
model2r = conv5_relu()
model3r = conv7_relu()

if args['cuda']:
    model1r.cuda()
    model2r.cuda()
    model3r.cuda()

train(train_loader, val_loader, model1r, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model2r, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model3r, num_epochs = args['ensemble_epochs'])

ensemble_relu = Ensemble([model1r, model2r, model3r], 3)

test(ensemble, val_loader)

ensemble_oe = Ensemble([model1oe, model2oe, model3oe], 3)

test(ensemble_oe, val_loader)

ensemble_5_oe = Ensemble([model1oe, model2oe, model3oe, model4oe, model5oe], 5)

test(ensemble_5_oe, val_loader)

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

"""# OOD detection

"""

def get_auroc(_pos, _neg):
    y_pos = np.ones(np.shape(_pos))
    y_neg = np.zeros(np.shape(_neg))
    y = np.concatenate((y_pos, y_neg))
    y_pos = np.concatenate((_pos, _neg))
    auroc_score = sk.roc_auc_score(y, y_pos)
    return auroc_score

def max_logit_anomaly_score(output):
    score = torch.max(output, axis = 1)[0]
    score = -1*score
    return score.cpu()

#maybe numerically unstable - fix
def max_softmax_anomaly_score(output):
    score = torch.exp(output)
    score = score / torch.sum(score, axis = 1, keepdims = True)
    score = torch.max(score, axis = 1)[0]
    score = -1*score 
    return score.cpu() 

def cross_entropy_anomaly_score(output):
    N, C = output.shape
    prosjek = torch.sum(output, axis = 1) / C
    score = torch.exp(output)
    score = torch.sum(score, axis = 1)
    score = torch.log(score)
    score = prosjek - score
    return score.cpu()

def perturbate(model, data, epsilon, temp):
  data.requires_grad = True
  output = model(data)
  output_n = output.detach().cpu().numpy()
  output_n = output_n / temp 
  output_n = output_n - np.max(output_n)   
  output_n = np.exp(output_n)
  output_n = output_n / np.sum(output_n)  
  label = np.argmax(output_n)
  label_ = torch.LongTensor([label]).cuda()
  label_ = Variable(label_)
  loss = F.cross_entropy(output/temp, label_)
  loss.backward()
  with torch.no_grad():
    grad = data.grad
    data_per = data-epsilon*torch.sign(grad)  
  model.zero_grad()
  return data_per

def auroc_score(ensemble, in_loader, ood_loader, method, odin = 0, epsilon = 0.002, temp = 10):
  in_score = []
  out_score = []

  def fill_score(myscore, myloader):
    for ind, (data, target) in enumerate(myloader):
      if (ind >= 200):
        break
      ubaci = data.clone()
      if args['cuda']:
        data, target, ubaci = data.cuda(), target.cuda(), ubaci.cuda()
      data, target, ubaci = Variable(data), Variable(target), Variable(ubaci)
      if odin == 1:
        for i in range (data.shape[0]):
          ubaci[i:i+1] = perturbate(ensemble, data[i:i+1], epsilon, temp)
      with torch.no_grad():
        output = ensemble(ubaci)
        if odin == 1:
          output = output / temp
        myscore.append(method(output).numpy()) 

  fill_score(in_score, in_loader)
  fill_score(out_score, ood_loader)

  """
  for ind, (data, target) in enumerate(in_loader):
    if (ind >= 200):
      break
    ubaci = data.clone()
    if args['cuda']:
      data, target, ubaci = data.cuda(), target.cuda(), ubaci.cuda()
    data, target, ubaci = Variable(data), Variable(target), Variable(ubaci)
    if odin == 1:
      for i in range (data.shape[0]):
        ubaci[i:i+1] = perturbate(ensemble, data[i:i+1], epsilon, temp)
    with torch.no_grad():
      output = ensemble(ubaci)
      if odin == 1:
        output = output / temp
      in_score.append(method(output).numpy()) 

  for ind, (data, target) in enumerate(ood_loader):
    if (ind >= 1000):
      break
    ubaci = data.clone()
    if args['cuda']:
      data, target, ubaci = data.cuda(), target.cuda(), ubaci.cuda()
    data, target, ubaci = Variable(data), Variable(target), Variable(ubaci)
    if odin == 1:
      for i in range (data.shape[0]):
        ubaci[i:i+1] = perturbate(ensemble, data[i:i+1], epsilon, temp)
    with torch.no_grad():
      output = ensemble(ubaci)
      if odin == 1:
        output = output / temp
      out_score.append(method(output).numpy()) 
  """

  in_score = np.concatenate(in_score)
  out_score = np.concatenate(out_score)

  return get_auroc(out_score, in_score)

print(auroc_score(ensemble, val_loader, ood_val_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3))

index =['AUROC max logit', 'AUROC softmax max', 'AUROC cross entropy', 'AUROC ODIN']
ood_report = pd.DataFrame(index=index)
def add_to_ood_report(col, calcs):
  ood_report[col] = pd.Series([calcs[k] for k in ('logit', 'softmax', 'cross', 'odin')], index=index)

calcs_ensemble_oe = {
    'logit' : auroc_score(ensemble_oe, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble_oe, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble_oe, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble_oe, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble OE', calcs_ensemble_oe)

calcs_ensemble = {
    'logit' : auroc_score(ensemble, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble', calcs_ensemble)

calcs_ensemble_5 = {
    'logit' : auroc_score(ensemble_5, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble_5, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble_5, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble_5, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble 5', calcs_ensemble_5)

calcs_ensemble_5_oe = {
    'logit' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble 5 OE', calcs_ensemble_5_oe)

ood_report

ood_report.to_latex()

"""# Adversarial Robustness

"""

#loading the data for the weak nn, make sure that you run the cell at the beginning that downloads mnist dataset
# load the data with validation set
adv_r_train_loader = torch.utils.data.DataLoader(
  train_subset,
  #transform=transforms.Normalize((0.1307,), (0.3081,)),
  batch_size = args['batch_size'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

adv_r_val_loader = torch.utils.data.DataLoader(
  val_subset,
  #transform=transforms.Normalize((0.1307,), (0.3081,)),
  batch_size = args['test_batch_size'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

adv_r_test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data',
                   train=False,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args['test_batch_size'], shuffle=True, **kwargs)

class WeakNet(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super(WeakNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, device=args['device'])
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, device=args['device'])
        self.conv2_drop = nn.Dropout2d()  #Dropout
        self.fc1 = nn.Linear(320, 50, device=args['device'])
        self.fc2 = nn.Linear(50, 10, device=args['device'])

    def forward(self, x):
        #Convolutional Layer/Pooling Layer/Activation
        x = F.relu(F.max_pool2d(self.conv1(x), 2)) 
        #Convolutional Layer/Dropout/Pooling Layer/Activation
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        #Fully Connected Layer/Activation
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        #Fully Connected Layer/Activation
        x = self.fc2(x)
        #Softmax gets probabilities. 
        return F.log_softmax(x, dim=1)

def train_weaknn(epoch, model):
    model.train()
    for batch_idx, (data, target) in enumerate(adv_r_train_loader):
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        #Variables in Pytorch are differenciable. 
        data, target = Variable(data), Variable(target)
        #This will zero out the gradients for this batch. 
        optimizer.zero_grad()
        output = model(data)
        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.
        loss = F.nll_loss(output, target)
        #dloss/dx for every Variable 
        loss.backward()
        #to do a one-step update on our parameter.
        optimizer.step()
        #Print out the loss periodically. 
        if batch_idx % args['log_interval'] == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(adv_r_train_loader.dataset),
                100. * batch_idx / len(adv_r_train_loader), loss.item()))

def test_weaknn(model):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in adv_r_val_loader:
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()

    test_loss /= len(adv_r_val_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(adv_r_val_loader.dataset),
        100. * correct / len(adv_r_val_loader.dataset)))

weakModel = WeakNet()
model = weakModel
if args['cuda']:
    weakModel.cuda()

optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])

for epoch in range(1, args['epochs'] + 5):
    train_weaknn(epoch, weakModel)
    test_weaknn(weakModel)

def adv_attack_nn(model, images, labels, verbose=False):
  def info(*args):
    if verbose:
      print(*args)
  predictions = model(images).argmax(axis=1).detach()
  results = []
  for Attack in (
      # L2
      fbattacks.L2BasicIterativeAttack,
      fbattacks.L2AdditiveGaussianNoiseAttack,
      fbattacks.L2DeepFoolAttack,
      fbattacks.L2FastGradientAttack,
      # Linf
      fbattacks.LinfFastGradientAttack,
      fbattacks.LinfPGD,
      # L0
      fbattacks.SaltAndPepperNoiseAttack,
  ):
    info('========', Attack.__name__, '=======\n')
    attack = Attack()
    epsilons = [0.3, 1.5, 10] #, 2.5, 5.0, 7.0]
    _, eps_clippedadv, eps_is_adv = attack(
        fmodel,
        images,
        labels,
        epsilons=epsilons
    )
    for eps, clippedadv, is_adv in zip(
        epsilons,
        eps_clippedadv, 
        eps_is_adv
      ):
      info(f' Epsilon: {eps}')
      robust_accuracy = 1 - is_adv.float().mean().item()
      info(f'  Robust accuracy {robust_accuracy*100:.2f}%')
      info('  Sample:')
      advindices = is_adv.long().nonzero().flatten().tolist()
      results += [[Attack.__name__, eps, robust_accuracy]]
      if advindices:
        indx = advindices[0]
    
        if verbose:
          plt.imshow(images[indx][0].cpu().numpy(), cmap='gray')
          plt.title('orig')
          plt.axis('off')
          plt.show()
          #fbplot.images(clippedadv[is_adv][:5])
          plt.imshow(clippedadv[indx][0].cpu().numpy(), cmap='gray')
          plt.title('adv')
          plt.axis('off')
          plt.show()
    
        info('   Prediction for usual:', predictions[indx].item())
        info('   Label:', labels[indx].item())
        advprediction = model(clippedadv[indx:indx+1]).detach().argmax(axis=1).item()
        info('   Prediction for adv.:', advprediction)
  return results

images, labels = next(iter(train_loader))
images = images.cuda()
labels = labels.cuda()

fmodel = FBModel(weakModel, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'CNN clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
cnn_res = adv_attack_nn(fmodel, images, labels)
index = [(e[0], e[1]) for e in cnn_res]
report = pd.DataFrame(index=index)
report['CNN'] = pd.Series([e[2] for e in cnn_res], index=index)
report

fmodel = FBModel(ensemble, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'Ensemble clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
ensemble_res = adv_attack_nn(fmodel, images, labels,)
report['Ensemble'] = pd.Series([e[2] for e in ensemble_res],index=index)
report

fmodel = FBModel(ensemble_5, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'Ensemble5 clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
ensemble_res = adv_attack_nn(fmodel, images, labels,)
report['Ensemble5'] = pd.Series([e[2] for e in ensemble_res],index=index)
report

fmodel = FBModel(ensemble_relu, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'EnsembleRELU clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
ensemble_res = adv_attack_nn(fmodel, images, labels,)
report['EnsembleRELU'] = pd.Series([e[2] for e in ensemble_res],index=index)
report

print('Median CNN, Ensemble: ', report['CNN'].median(), report['Ensemble'].median())
print('Median CNN, Ensemble5: ', report['CNN'].median(), report['Ensemble5'].median())
print('Mean:', (report['CNN'] - report['Ensemble']).mean())
print('Mean:', (report['CNN'] - report['Ensemble5']).mean())

def get_formatted(r):
  f = r.copy()
  f.index = [f"{e[0]}, Îµ={e[1]}" for e in f.index]
  f['CNN'] = [f"{e*100:.2f}%" for e in f['CNN']]
  f['Ensemble'] = [f"{e*100:.2f}%" for e in f['Ensemble']]
  f['Ensemble5'] = [f"{e*100:.2f}%" for e in f['Ensemble5']]
  return f
freport = get_formatted(report)
freport

freport.to_latex()

"""## Adversarial Training

"""

import copy 
report = pd.DataFrame(index=index)
adv_train_subset, adv_val_subset, _ = torch.utils.data.random_split(
  mnist_train,
  [int(mnist_train_n * 1/12), int(mnist_train_n*1/12), int(mnist_train_n*10/12)],  # Adv. example generation is slow. So take a few.
  generator=torch.Generator().manual_seed(1)
)

adv_train_loader =  torch.utils.data.DataLoader(
  adv_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
adv_val_loader =  torch.utils.data.DataLoader(
  adv_val_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
epsilon = 1.5
at_epochs = 3
attack = fbattacks.LinfProjectedGradientDescentAttack()

for at_lambda in (0.2, 0.75):
  print(f"------\n Lambda={at_lambda} \n ------\n")

  for innerm in (model1, model2, model3): #, model4, model5):
    innermodelcopy = copy.deepcopy(innerm)
    mymodel = FBModel(innermodelcopy, bounds=(images.min().item(),images.max().item()), device='cuda')
    #mymodel = FBModel(innerm, bounds=(images.min().item(),images.max().item()), device='cuda')
    def adv_attack(images, labels):
      global mymodel
      _, clippedadv, _ = attack(
          mymodel,
          images,
          labels,
          epsilons=epsilon
      )
      return clippedadv
    train(
        adv_train_loader,
        adv_val_loader, 
        innerm, 
        num_epochs = at_epochs, 
        adversarial_attack=adv_attack, 
        adversarial_lambda=at_lambda
      )

  fmodel = FBModel(ensemble, bounds=(images.min().item(),images.max().item()), device='cuda')
  print(f'AT Ensemble clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
  ensemble_res = adv_attack_nn(fmodel, images, labels, verbose=True)
  report[f'AT Ensemble {at_lambda} {at_epochs}'] = pd.Series([e[2] for e in ensemble_res],index=index)
report





"""# Calibration"""

bins_ints = list(e/10 for e in range(0, 10, 1))
bins_ints

def calc_calibration(model):
  EPSILON = 0.0000001
  count = 0
  bins = {b : defaultdict(float) for b in bins_ints}
  for images, labels in train_loader:
    count += len(images)
    images = images.cuda()
    labels = labels.cuda()
    if labels.shape 
    predictions = model(images).softmax(axis=1)
    for ps, l in zip(predictions, labels):
      maxp = ps.max(axis=0)
      plabel, prob  = maxp.indices.item(), maxp.values.item()
      mybin = bins[int(prob * 10) / 10.0]
      mybin['psum'] += prob
      mybin['msum'] += float(l == plabel)
      mybin['count'] += 1.0
  for k in bins:
    b = bins[k]
    b['rms_error'] = (b['psum'] - b['msum'])**2 / (b['count'] + EPSILON)/ (count + EPSILON)
    b['accuracy'] = b['msum'] / (b['count'] + EPSILON)
    b['confidence'] = b['psum'] / (b['count'] + EPSILON)
    b['ece'] = abs(b['accuracy'] - b['confidence']) * b['count'] / (count + EPSILON)
    b['mce'] = abs(b['accuracy'] - b['confidence'])
  rms_error = sum(bins[k]['rms_error'] for k in bins)**.5  # TODO: See below results. Why not changed?
  ece = sum(bins[k]['ece'] for k in bins)
  mce = max(bins[k]['mce'] for k in bins)
  calcs = {
      'rms_error': rms_error,
      'ece': ece,
      'mce': mce,
      'bins': bins,
      'count': count
  }
  return calcs

def print_calibration_report(calcs):
  bins = calcs['bins']
  print(f"RMS error: {calcs['rms_error']}")
  print(f"ECE: {calcs['ece']}")
  print(f"MCE: {calcs['mce']}")
  r = pd.DataFrame(index=bins_ints)
  r['score'] = pd.Series([bins[k]['res'] for k in bins_ints], index=bins_ints)
  r['count'] = pd.Series([bins[k]['count'] for k in bins_ints], index=bins_ints)
  r['msum'] = pd.Series([bins[k]['msum'] for k in bins_ints], index=bins_ints)
  r['psum'] = pd.Series([bins[k]['psum'] for k in bins_ints], index=bins_ints)
  r['accuracy'] = pd.Series([bins[k]['accuracy'] for k in bins_ints], index=bins_ints)
  r.loc['Total'] = r.sum(numeric_only=True)
  def accuracybar(r):
    r = r.copy()
    r = r.drop('Total')
    index = bins_ints
    r2 = pd.DataFrame({
        'accuracy': r['accuracy'],
        'bins': bins_ints
      },
      index=bins_ints
    )
    r2.plot.bar()
    plt.show()
  def density(r):
    r = r.copy()
    r = r.drop('Total')
    index = bins_ints
    r2 = pd.DataFrame({
        '% of samples': r['count']/calcs['count'],
        'bins': bins_ints
      },
      index=bins_ints
    )
    r2.plot.bar()
    plt.show()
  accuracybar(r)
  density(r)

index =['RMS', 'ECE', 'MCE']
calibration_report = pd.DataFrame(index=index)
def add_to_report(col, calcs):
  calibration_report[col] = pd.Series([calcs[k] for k in ('rms_error', 'ece', 'mce')], index=index)

calcs = calc_calibration(weakModel)
print_calibration_report(calcs)
add_to_report('CNN', calcs)

calcs = calc_calibration(ensemble)
print_calibration_report(calcs)
add_to_report('Vanilla Ensemble', calcs)

calcs = calc_calibration(ensemble5)
print_calibration_report(calcs)
add_to_report('Ensemble5', calcs)

temperatura = [0.7, 0.8, 0.9, 1, 2, 3]

for temp in temperatura:
  ensemble = Ensemble([model1, model2, model3], temp)
  calcs = calc_calibration(ensemble)
  print("Eto temperatura: ", temp)
  print_calibration_report(calcs)
  add_to_report(f'Ensemble. T={temp}', calcs)

temperatura = [3, 5, 10]

for temp in temperatura:
  ensemble = Ensemble([model1, model2, model3, model4, model5], temp)
  calcs = calc_calibration(ensemble)
  print("Eto temperatura: ", temp)
  print_calibration_report(calcs)
  add_to_report(f'Ens5. T={temp}', calcs)

calibration_report

"""# Distribution shift

"""

!wget https://zenodo.org/record/3239543/files/mnist_c.zip
!unzip mnist_c.zip

brightness_train_data = np.load("/content/mnist_c/brightness/train_images.npy")
brightness_train_labels = np.load("/content/mnist_c/brightness/train_labels.npy")
 
canny_train_data = np.load("/content/mnist_c/canny_edges/train_images.npy")
canny_train_labels = np.load("/content/mnist_c/canny_edges/train_labels.npy")
 
dotted_line_train_data = np.load("/content/mnist_c/dotted_line/train_images.npy")
dotted_line_train_labels = np.load("/content/mnist_c/dotted_line/train_labels.npy")
 
fog_train_data = np.load("/content/mnist_c/fog/train_images.npy")
fog_train_labels = np.load("/content/mnist_c/fog/train_labels.npy")
 
glass_blur_train_data = np.load("/content/mnist_c/glass_blur/train_images.npy")
glass_blur_train_labels = np.load("/content/mnist_c/glass_blur/train_labels.npy")
 
identity_train_data = np.load("/content/mnist_c/identity/train_images.npy")
identity_train_labels = np.load("/content/mnist_c/identity/train_labels.npy")
 
impulse_noise_train_data = np.load("/content/mnist_c/impulse_noise/train_images.npy")
impulse_noise_train_labels = np.load("/content/mnist_c/impulse_noise/train_labels.npy")

brightness_test_data = np.load("/content/mnist_c/brightness/test_images.npy")
brightness_test_labels = np.load("/content/mnist_c/brightness/test_labels.npy")

canny_test_data = np.load("/content/mnist_c/canny_edges/test_images.npy")
canny_test_labels = np.load("/content/mnist_c/canny_edges/test_labels.npy")

dotted_line_test_data = np.load("/content/mnist_c/dotted_line/test_images.npy")
dotted_line_test_labels = np.load("/content/mnist_c/dotted_line/test_labels.npy")

fog_test_data = np.load("/content/mnist_c/fog/test_images.npy")
fog_test_labels = np.load("/content/mnist_c/fog/test_labels.npy")

glass_blur_test_data = np.load("/content/mnist_c/glass_blur/test_images.npy")
glass_blur_test_labels = np.load("/content/mnist_c/glass_blur/test_labels.npy")

identity_test_data = np.load("/content/mnist_c/identity/test_images.npy")
identity_test_labels = np.load("/content/mnist_c/identity/test_labels.npy")

impulse_noise_test_data = np.load("/content/mnist_c/impulse_noise/test_images.npy")
impulse_noise_test_labels = np.load("/content/mnist_c/impulse_noise/test_labels.npy")

brightness_train_data = torch.from_numpy(brightness_train_data).cuda()
brightness_train_labels = torch.from_numpy(brightness_train_labels).cuda()
 
canny_train_data = torch.from_numpy(canny_train_data).cuda()
canny_train_labels = torch.from_numpy(canny_train_labels).cuda()
 
dotted_line_train_data = torch.from_numpy(dotted_line_train_data).cuda()
dotted_line_train_labels = torch.from_numpy(dotted_line_train_labels).cuda()
 
fog_train_data = torch.from_numpy(fog_train_data).cuda()
fog_train_labels = torch.from_numpy(fog_train_labels).cuda()
 
glass_blur_train_data = torch.from_numpy(glass_blur_train_data).cuda()
glass_blur_train_labels = torch.from_numpy(glass_blur_train_labels).cuda()
 
identity_train_data = torch.from_numpy(identity_train_data).cuda()
identity_train_labels = torch.from_numpy(identity_train_labels).cuda()
 
impulse_noise_train_data = torch.from_numpy(impulse_noise_train_data).cuda()
impulse_noise_train_labels = torch.from_numpy(impulse_noise_train_labels).cuda()

brightness_test_data = torch.from_numpy(brightness_test_data).cuda()
brightness_test_labels = torch.from_numpy(brightness_test_labels).cuda()

canny_test_data = torch.from_numpy(canny_test_data).cuda()
canny_test_labels = torch.from_numpy(canny_test_labels).cuda()

dotted_line_test_data = torch.from_numpy(dotted_line_test_data).cuda()
dotted_line_test_labels = torch.from_numpy(dotted_line_test_labels).cuda()

fog_test_data = torch.from_numpy(fog_test_data).cuda()
fog_test_labels = torch.from_numpy(fog_test_labels).cuda()

glass_blur_test_data = torch.from_numpy(glass_blur_test_data).cuda()
glass_blur_test_labels = torch.from_numpy(glass_blur_test_labels).cuda()

identity_test_data = torch.from_numpy(identity_test_data).cuda()
identity_test_labels = torch.from_numpy(identity_test_labels).cuda()

impulse_noise_test_data = torch.from_numpy(impulse_noise_test_data).cuda()
impulse_noise_test_labels = torch.from_numpy(impulse_noise_test_labels).cuda()

brightness_train_labels = torch.nn.functional.one_hot(brightness_train_labels)
canny_train_labels = torch.nn.functional.one_hot(canny_train_labels)
dotted_line_train_labels = torch.nn.functional.one_hot(dotted_line_train_labels)
fog_train_labels = torch.nn.functional.one_hot(fog_train_labels)
glass_blur_train_labels = torch.nn.functional.one_hot(glass_blur_train_labels)
identity_train_labels = torch.nn.functional.one_hot(identity_train_labels)
impulse_noise_train_labels = torch.nn.functional.one_hot(impulse_noise_train_labels)

brightness_test_labels = torch.nn.functional.one_hot(brightness_test_labels)
canny_test_labels = torch.nn.functional.one_hot(canny_test_labels)
dotted_line_test_labels = torch.nn.functional.one_hot(dotted_line_test_labels)
fog_test_labels = torch.nn.functional.one_hot(fog_test_labels)
glass_blur_test_labels = torch.nn.functional.one_hot(glass_blur_test_labels)
identity_test_labels = torch.nn.functional.one_hot(identity_test_labels)
impulse_noise_test_labels = torch.nn.functional.one_hot(impulse_noise_test_labels)

def convert_to_shape(data, labels):
  new_structure = []
  for i in range(len(data)):
    instance = data[0].squeeze(2).unsqueeze(0)
    new_structure.append((instance, labels[i]))
  return new_structure

brightness_train = convert_to_shape(brightness_train_data, brightness_train_labels)
canny_train = convert_to_shape(canny_train_data, canny_train_labels)
dotted_line_train =convert_to_shape(dotted_line_train_data, dotted_line_train_labels)
fog_train =convert_to_shape(fog_train_data, fog_train_labels)
glass_blur_train =convert_to_shape(glass_blur_train_data, glass_blur_train_labels)
identity_train =convert_to_shape(identity_train_data, identity_train_labels)
impulse_noise_train =convert_to_shape(impulse_noise_train_data, impulse_noise_train_labels)

brightness_test = convert_to_shape(brightness_test_data, brightness_test_labels)
canny_test = convert_to_shape(canny_test_data, canny_test_labels)
dotted_line_test =convert_to_shape(dotted_line_test_data, dotted_line_test_labels)
fog_test =convert_to_shape(fog_test_data, fog_test_labels)
glass_blur =convert_to_shape(glass_blur_test_data, glass_blur_test_labels)
identity_test =convert_to_shape(identity_test_data, identity_test_labels)
impulse_noise_test =convert_to_shape(impulse_noise_test_data, impulse_noise_test_labels)

brightness_train_subset, brightness_val_subset = torch.utils.data.random_split(
  brightness_train,
  [int(len(brightness_train) * 5/6), int(len(brightness_train) * 1/6)],
  generator=torch.Generator().manual_seed(1)
)

canny_train_subset, canny_val_subset = torch.utils.data.random_split(
  canny_train,
  [int(len(canny_train) * 5/6), int(len(canny_train) * 1/6)],
  generator=torch.Generator().manual_seed(1)
)

dotted_line_train_subset, dotted_line_val_subset = torch.utils.data.random_split(
  dotted_line_train,
  [int(len(dotted_line_train) * 5/6), int(len(dotted_line_train) * 1/6)],
  generator=torch.Generator().manual_seed(1)
)

fog_train_subset, fog_val_subset = torch.utils.data.random_split(
  fog_train,
  [int(len(fog_train) * 5/6), int(len(fog_train) * 1/6)],
  generator=torch.Generator().manual_seed(1)
)

glass_blur_train_subset, glass_blur_val_subset = torch.utils.data.random_split(
  glass_blur_train,
  [int(len(glass_blur_train) * 5/6), int(len(glass_blur_train) * 1/6)],
  generator=torch.Generator().manual_seed(1)
)

identity_train_subset, identity_val_subset = torch.utils.data.random_split(
  identity_train,
  [int(len(identity_train) * 5/6), int(len(identity_train) * 1/6)],
  generator=torch.Generator().manual_seed(1)
)

impulse_noise_train_subset, impulse_noise_val_subset = torch.utils.data.random_split(
  impulse_noise_train,
  [int(len(impulse_noise_train) * 5/6), int(len(impulse_noise_train) * 1/6)],
  generator=torch.Generator().manual_seed(1)
)

brightness_loader = torch.utils.data.DataLoader(
  brightness_train_subset,
  batch_size = args['batch_size_outlier'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
brightness_val_loader = torch.utils.data.DataLoader(
  brightness_val_subset,
  batch_size = args['batch_size_ood'], 
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
brightness_test_loader = torch.utils.data.DataLoader(
  brightness_test,
  batch_size = args['batch_size_ood'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

"""Noise training"""

mnist_train_noise_10
mnist_train_noise_10_n = len(mnist_train_noise_10)
print('mnist_train_noise_15_n', mnist_train_noise_10_n)
noise_10_train_subset, noise_10_val_subset = torch.utils.data.random_split(
  mnist_train_noise_10,
  [int(math.floor(mnist_train_noise_10_n * 5/6)), int(math.ceil(mnist_train_noise_10_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
noise_10_train_loader = torch.utils.data.DataLoader(
  noise_10_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
noise_10_val_loader = torch.utils.data.DataLoader(
  noise_10_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_noise_15
mnist_train_noise_15_n = len(mnist_train_noise_15)
print('mnist_train_noise_15_n', mnist_train_noise_15_n)
noise_15_train_subset, noise_15_val_subset = torch.utils.data.random_split(
  mnist_train_noise_15,
  [int(math.floor(mnist_train_noise_15_n * 5/6)), int(math.ceil(mnist_train_noise_15_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
noise_15_train_loader = torch.utils.data.DataLoader(
  noise_15_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
noise_15_val_loader = torch.utils.data.DataLoader(
  noise_15_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_noise_20
mnist_train_noise_20_n = len(mnist_train_noise_20)
print('mnist_train_noise_20_n', mnist_train_noise_20_n)
noise_20_train_subset, noise_20_val_subset = torch.utils.data.random_split(
  mnist_train_noise_20,
  [int(math.floor(mnist_train_noise_20_n * 5/6)), int(math.ceil(mnist_train_noise_20_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)

noise_20_train_loader = torch.utils.data.DataLoader(
  noise_20_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

noise_20_val_loader = torch.utils.data.DataLoader(
  noise_20_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_noise_25
mnist_train_noise_25_n = len(mnist_train_noise_25)
print('mnist_train_noise_25_n', mnist_train_noise_25_n)
noise_25_train_subset, noise_25_val_subset = torch.utils.data.random_split(
  mnist_train_noise_25,
  [int(math.floor(mnist_train_noise_25_n * 5/6)), int(math.ceil(mnist_train_noise_25_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
noise_25_train_loader = torch.utils.data.DataLoader(
  noise_25_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
noise_25_val_loader = torch.utils.data.DataLoader(
  noise_25_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_noise_30
mnist_train_noise_30_n = len(mnist_train_noise_30)
print('mnist_train_noise_30_n', mnist_train_noise_30_n)
noise_30_train_subset, noise_30_val_subset = torch.utils.data.random_split(
  mnist_train_noise_30,
  [int(math.floor(mnist_train_noise_30_n * 5/6)), int(math.ceil(mnist_train_noise_30_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
noise_30_train_loader = torch.utils.data.DataLoader(
  noise_30_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
noise_30_val_loader = torch.utils.data.DataLoader(
  noise_30_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_10_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_10_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_10_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_10_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_10_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_15_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_20_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_25_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_30_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)



"""MixUp Training"""

mnist_train_mixup_10
mnist_train_mixup_10_n = len(mnist_train_mixup_10)
print('mnist_train_mixup_15_n', mnist_train_mixup_10_n)
mixup_10_train_subset, mixup_10_val_subset = torch.utils.data.random_split(
  mnist_train_mixup_10,
  [int(math.floor(mnist_train_mixup_10_n * 5/6)), int(math.ceil(mnist_train_mixup_10_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
mixup_10_train_loader = torch.utils.data.DataLoader(
  mixup_10_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
mixup_10_val_loader = torch.utils.data.DataLoader(
  mixup_10_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_mixup_15
mnist_train_mixup_15_n = len(mnist_train_mixup_15)
print('mnist_train_mixup_15_n', mnist_train_mixup_15_n)
mixup_15_train_subset, mixup_15_val_subset = torch.utils.data.random_split(
  mnist_train_mixup_15,
  [int(math.floor(mnist_train_mixup_15_n * 5/6)), int(math.ceil(mnist_train_mixup_15_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
mixup_15_train_loader = torch.utils.data.DataLoader(
  mixup_15_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
mixup_15_val_loader = torch.utils.data.DataLoader(
  mixup_15_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_mixup_20
mnist_train_mixup_20_n = len(mnist_train_mixup_20)
print('mnist_train_mixup_20_n', mnist_train_mixup_20_n)
mixup_20_train_subset, mixup_20_val_subset = torch.utils.data.random_split(
  mnist_train_mixup_20,
  [int(math.floor(mnist_train_mixup_20_n * 5/6)), int(math.ceil(mnist_train_mixup_20_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
mixup_20_train_loader = torch.utils.data.DataLoader(
  mixup_20_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
mixup_20_val_loader = torch.utils.data.DataLoader(
  mixup_20_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_mixup_25
mnist_train_mixup_25_n = len(mnist_train_mixup_25)
print('mnist_train_mixup_25_n', mnist_train_mixup_25_n)
mixup_25_train_subset, mixup_25_val_subset = torch.utils.data.random_split(
  mnist_train_mixup_25,
  [int(math.floor(mnist_train_mixup_25_n * 5/6)), int(math.ceil(mnist_train_mixup_25_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
mixup_25_train_loader = torch.utils.data.DataLoader(
  mixup_25_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
mixup_25_val_loader = torch.utils.data.DataLoader(
  mixup_25_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

mnist_train_mixup_30
mnist_train_mixup_30_n = len(mnist_train_mixup_30)
print('mnist_train_mixup_30_n', mnist_train_mixup_30_n)
mixup_30_train_subset, mixup_30_val_subset = torch.utils.data.random_split(
  mnist_train_mixup_30,
  [int(math.floor(mnist_train_mixup_30_n * 5/6)), int(math.ceil(mnist_train_mixup_30_n*1/6))],
  generator=torch.Generator().manual_seed(1)
)
 
mixup_30_train_loader = torch.utils.data.DataLoader(
  mixup_30_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
 
mixup_30_val_loader = torch.utils.data.DataLoader(
  mixup_30_val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)



model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(mixup_10_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(mixup_10_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(mixup_10_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(mixup_10_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(mixup_10_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_15_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_15_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_20_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_20_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_25_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_25_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(noise_30_train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(noise_30_train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

