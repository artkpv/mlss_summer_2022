# -*- coding: utf-8 -*-
"""mlss_final_proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wNfSCzcdd8xgfrSZD_Fh18yNMt4wb_23

# MLSS final project. Safe MNIST classifier.
"""

!pip install torch torchvision foolbox opendatasets pandas

#Import Libraries


from __future__ import print_function
import argparse
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import torchvision.transforms as T
from torch.autograd import Variable
import matplotlib.pyplot as plt
import numpy as np
from foolbox import PyTorchModel as FBModel, attacks as fbattacks, utils as fbutils, plot as fbplot, criteria as fbcriteria
import pandas as pd
import opendatasets as od
import sklearn.metrics as sk
import os
from collections import defaultdict
from PIL import Image

args={}
if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')        

kwargs={}
args['batch_size_conv']=120
args['test_batch_size_conv']=120
args['batch_size_ood']=1
args['batch_size_outlier'] = 120
args['batch_size']=1000
args['test_batch_size']=1000
args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. 
args['ensemble_epochs'] = 3
args['lr']=0.01 #Learning rate is how fast it will decend.
args['lr_conv']=0.001 #Learning rate is how fast it will decend.
args['weight_decay']=0.999 #L2 penalty
args['gamma']=0.98 #exponential decay of our learning rate
args['momentum']=0.5 
args['num_workers'] = 10
args['seed']=1 #random seed
args['log_interval']=10
args['cuda']= args['device'] == torch.device('cuda')

"""# Load data"""

#download the mnist dataset for training
mnist_train = datasets.MNIST(
    '../data',
    train=True,
    download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        ])
)
mnist_train_n = len(mnist_train)
print('mnist_train_n', mnist_train_n)
train_subset, val_subset = torch.utils.data.random_split(
  mnist_train,
  [int(mnist_train_n * 5/6), int(mnist_train_n*1/6)],
  generator=torch.Generator().manual_seed(1)
)

od.download("https://www.kaggle.com/datasets/lubaroli/notmnist")

!tar -xvf /content/notmnist/notMNIST_small.tar.gz

os.remove('./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png')
os.remove('./notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png')

notmnist = datasets.ImageFolder('./notMNIST_small/', transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor()]))

notmnist_n = len(notmnist)

val_ood, test_ood = torch.utils.data.random_split(
  notmnist,
  [int(notmnist_n * 1/2), int(notmnist_n * 1/2)],
  generator=torch.Generator().manual_seed(1)
)

ood_val_loader = torch.utils.data.DataLoader(
  val_ood,
  batch_size = args['batch_size_ood'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

ood_test_loader = torch.utils.data.DataLoader(
  test_ood,
  batch_size = args['batch_size_ood'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

fashion_train = datasets.FashionMNIST('./data', train=True, download=True, transform=transforms.ToTensor())
fashion_test = datasets.FashionMNIST('./data', train=False, download=True, transform=transforms.ToTensor())
fashion_train_n = len(fashion_train)
fashion_train_subset, fashion_val_subset = torch.utils.data.random_split(
  fashion_train,
  [int(fashion_train_n * 5/6), int(fashion_train_n*1/6)],
  generator=torch.Generator().manual_seed(1)
)
fashion_test = datasets.FashionMNIST('./data', train=False, download=True, transform=transforms.ToTensor())

outlier_loader = torch.utils.data.DataLoader(
  fashion_train_subset,
  batch_size = args['batch_size_outlier'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

outlier_val_loader = torch.utils.data.DataLoader(
  fashion_val_subset,
  batch_size = args['batch_size_ood'], 
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

outlier_test_loader = torch.utils.data.DataLoader(
  fashion_test,
  batch_size = args['batch_size_ood'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

"""# Data Augmentation

Gaussian Noise
"""

def gaussian_noise(train_data, perc_noisy=100):
    num_noisy = len(train_data) * int(perc_noisy/100)
    noisy_train_data = []; 
    print(num_noisy)
    idx = np.random.randint(0, len(train_data), num_noisy)   
    for i in range(len(train_data)):
      if i in idx:
        noise_multiplier = torch.rand(1)
        noisy_example = train_data[i][0] + noise_multiplier/2 * torch.randn(28,28)
      else:
        noise_multiplier = 0
        noisy_example = train_data[i][0]
      label = train_data[i][1]
      noisy_train_data.append((noisy_example,label))
    return noisy_train_data

mnist_train_dataug_noise = gaussian_noise(mnist_train, perc_noisy = 10)

for i in range(9):  
  plt.subplot(330 + 1 + i)  
  plt.imshow(mnist_train_dataug_noise[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()

"""Rotation (and Gaussian Noise)"""

def random_rotation(train_data, perc_rotated=100, randomNoise=False):
    num_rotated = len(train_data) * perc_rotated/100
    rotated_train_data = []; 
    rotater = T.RandomRotation(degrees=(0, 180))
    idx = np.random.randint(0, len(train_data), int(num_rotated))  

    for i in range(len(train_data)):
      if i in idx:
        rotated_example = rotater(train_data[i][0])
        if randomNoise:
          noise_multiplier = torch.rand(1)
          rotated_example = rotated_example + noise_multiplier/2 * torch.randn(28,28)
          label = train_data[i][1]
      else:
        rotated_example = train_data[i][0]
        if randomNoise:
          noise_multiplier = torch.rand(1)
          rotated_example = rotated_example + noise_multiplier/2 * torch.randn(28,28)
        label = train_data[i][1]
      rotated_train_data.append((rotated_example,label))
    return rotated_train_data

mnist_train_dataug_rot = random_rotation(mnist_train, perc_rotated = 75, randomNoise=True)

for i in range(9):  
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_dataug_rot[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_dataug_rot[i][1])

"""Flipping"""

def flipping(train_data):
  
  p_h = torch.randn(1)
  hflipper = T.RandomHorizontalFlip(p_h)
  p_v = torch.randn(1)
  vflipper = T.RandomVerticalFlip(p_v)

  flipped_train_data = []

  for i in range(len(train_data)):
    flipped_1 = hflipper(train_data[i][0])
    flipped_2 = vflipper(flipped_1)
    label = train_data[i][1]
    flipped_train_data.append((flipped_2, label))
  return flipped_train_data

mnist_train_dataug_flip = flipping(mnist_train)

for i in range(9):  
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_dataug_flip[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_dataug_flip[i][1])

"""Inversion"""

def inversion(train_data, perc_inverted=100):
  
  inverter = T.RandomInvert()
  inverted_train_data = []
  num_inv = len(train_data) * perc_inverted/100
  idx = np.random.randint(0, len(train_data), int(num_inv))  
  for i in range(len(train_data)):
    if i in idx:
      inverted_img = inverter(train_data[i][0]) 
      label = train_data[i][1]
      inverted_train_data.append((inverted_img, label))
    else:
      inverted_train_data.append((train_data[i][0],train_data[i][1]))
  return   inverted_train_data

mnist_train_inverted = inversion(mnist_train, perc_inverted=50)

for i in range(9):  
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_inverted[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_inverted[i][1])

"""MixUp"""

def mixup(train_data, perc_mixed=100):
    
    num_mixed = len(train_data) * perc_mixed/100
    images = []; labels = [];
    mixup_data=[];
    for i in range(int(num_mixed)):
        idx = np.random.randint(0, num_mixed)
        # Create a one hot label
        label = torch.zeros(10)
        # Set label accurately for the instance chosen 
        label[train_data[idx][1]] = 1.

        image = train_data[idx][0]

        
        # Choose another image/label randomly
        mixup_idx = np.random.randint(0, len(train_data))
        mixup_label = torch.zeros(10)
        mixup_label[train_data[mixup_idx][1]] = 1.
        mixup_image = train_data[mixup_idx][0]
        # Select a random number from the given beta distribution
        # Mixup the images accordingly
        alpha = 0.2
        lam = np.random.beta(alpha, alpha)
        image = lam * image + (1 - lam) * mixup_image
        label = lam * label + (1 - lam) * mixup_label
        
        images.append(image)
        labels.append(label)
        mixup_data.append((image,label))
    return mixup_data

mnist_train_mixup = mixup(mnist_train, 100)

for i in range(9):
  plt.subplot(330 + 1 + i)
  plt.imshow(mnist_train_mixup[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
  plt.show()
  print(mnist_train_mixup[i][1])

"""CutMix"""

def rand_bbox(size, lam):
    """ Generate random bounding box 
    Args:
        - size: [width, breadth] of the bounding box
        - lamb: (lambda) cut ratio parameter
    Returns:
        - Bounding box
    """
    W = size[0]
    H = size[1]
    cut_ratio = np.sqrt(1. - lam)
    cut_w = np.int(W * cut_ratio)
    cut_h = np.int(H * cut_ratio)

    # uniform
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error  # Skipping. TODO
# def generate_cutmix_image(train_data, beta):
#     """ Generate a CutMix augmented image from a batch 
#     Args:
#         - image_batch: a batch of input images
#         - image_batch_labels: labels corresponding to the image batch
#         - beta: a parameter of Beta distribution.
#     Returns:
#         - CutMix image batch, updated labels
#     """
#     labels = np.zeros(len(train_data))
# 
#     # generate mixed sample
#     cutmix_examples = []
#     for i in range(math.floor(len(train_data))):
#       lam = np.random.beta(beta, beta)
#       idx_a = np.random.randint(len(train_data))
#       idx_b = np.random.randint(len(train_data))
#       img_a = train_data[idx_a][0]
#       img_b = train_data[idx_b][0]
#       target_a = train_data[idx_a][1]
#       target_b = train_data[idx_b][1]
#       bbx1, bby1, bbx2, bby2 = rand_bbox(train_data[idx_a][0].squeeze(0).shape, lam)
#       #train_data_updated = train_data.copy()
#       train_data_updated = train_data.copy()
# 
#       img_a_box = img_a[bbx1:bbx2, bby1:bby2]
#       img_b_box = img_b[bbx1:bbx2, bby1:bby2]
#       img_a[bbx1:bbx2, bby1:bby2] = img_b_box
#       img_b[bbx1:bbx2, bby1:bby2] = img_a_box
#       train_data_updated[idx_a][0] = img_a
#       train_data_updated[idx_b][0] = img_b
# 
#       # adjust lambda to exactly match pixel ratio
#       lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (28 * 28))
#       label = target_a * lam + target_b * (1. - lam)
#       labels[idx_a] = label
#       cutmix_examples.append((training_data_updated[idx_a][0],label))
#     return cutmix_examples
#     #return train_data_updated, labels
# 
# # Generate CutMix image
# # Let's use the first image of the batch as the input image to be augmented
# mnist_train_cutmix = generate_cutmix_image(mnist_train_list, 1.0)
# 
# # Show original images
# #print("Original Images")
# #for i in range(2):
# #    for j in range(2):
# #        plt.subplot(2,2,2*i+j+1)
# #        plt.imshow(training_data_updated[2*i+j])
# #plt.show()''
# 
# # Show CutMix images
# 
# for i in range(9):
#   plt.subplot(330 + 1 + i)
#   plt.imshow(mnist_train_cutmix[i][0].squeeze(0), cmap=plt.get_cmap('gray'))
#   plt.show()
#   print(mnist_train_cutmix[i][1])
# 
# # Show CutMix images
# #print("CutMix Images")
# #for i in range(2):
# #    for j in range(2):
# #        plt.subplot(2,2,2*i+j+1)
# #        plt.imshow(training_data_updated[2*i+j])
# #plt.show()
# 
# # Print labels
# #print('Original labels:')
# #print(train_batch_labels)
# #print('Updated labels')
# #print(image_batch_labels_updated)

"""PixMix"""

# Commented out IPython magic to ensure Python compatibility.
# %%script false --no-raise-error  # Skipping. TODO
# # We need to download mixing set of fractals 
# 
# def pixmix(x_orig, x_mixing_pic, k=4, beta=3):
#   x_pixmix = np.random.choice([augment(x_orig), x_orig])
# 
#   # random count of mixing rounds
#   rounds = np.linspace(0,k+1)
#   for i in range(np.random.choice[rounds]):
# 
#     mix_image = np.random.choice([augment(x_orig), x_mixing_pic])
#     # need to add addition and multiplication operations 
#     mix_op = np.random.choice([addition(), multiplication()])
# 
#     x_pixmix = mix_op(x_pixmix, mix_image, beta)
# 
# def augment(x):
# 
#   # need to add different augmentation operations 
#   aug_op = np.random.choice([transforms.RandomInvert(), transforms.RandomPosterize(), transforms.RandomSolarize())
#   return aug_op(x)
# 
# def addition(x_1, x_2, magnitude):
#   return x_1 + magnitude * x_2 
# 
# def multiplication(x_1, x_2, magnitude):
#   return x_1 @ (magnitude * x_2)
#

"""# CNN ensemble

In this section we will implement three convolutional neural networks and combine them in an ensemble as in this paper: https://arxiv.org/pdf/2008.10400v2.pdf
"""

# load the data with validation set for conv ensemble

train_loader = torch.utils.data.DataLoader(
  train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

val_loader = torch.utils.data.DataLoader(
  val_subset,
  batch_size = args['test_batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data',
                   train=False,
                   transform=transforms.Compose([
                       transforms.ToTensor()
                   ])),
    batch_size=args['test_batch_size_conv'], shuffle=True, **kwargs)

"""## Architecture"""

#as in the paper, except GELU instead of RELU
class conv3(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 48, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(48),
            nn.GELU(), 

            nn.Conv2d(48, 64, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(),

            nn.Conv2d(64, 80, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(80),
            nn.GELU(),

            nn.Conv2d(80, 96, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(),

            nn.Conv2d(96, 112, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(112),
            nn.GELU(),

            nn.Conv2d(112, 128, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(128),
            nn.GELU(),

            nn.Conv2d(128, 144, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(144),
            nn.GELU(),

            nn.Conv2d(144, 160, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(160),
            nn.GELU(),

            nn.Conv2d(160, 176, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(176),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(11264, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

class conv3_relu(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(32),
            nn.ReLU(), 

            nn.Conv2d(32, 48, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(48),
            nn.ReLU(), 

            nn.Conv2d(48, 64, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.Conv2d(64, 80, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(80),
            nn.ReLU(),

            nn.Conv2d(80, 96, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(96),
            nn.ReLU(),

            nn.Conv2d(96, 112, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(112),
            nn.ReLU(),

            nn.Conv2d(112, 128, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.Conv2d(128, 144, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(144),
            nn.ReLU(),

            nn.Conv2d(144, 160, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(160),
            nn.ReLU(),

            nn.Conv2d(160, 176, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(176),
            nn.ReLU(),

            nn.Flatten(1, -1),
            nn.Linear(11264, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#trains faster, still 99% acc in first epoch!
class conv3_(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 48, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(48),
            nn.GELU(), 

            nn.Conv2d(48, 64, kernel_size = 3, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(30976, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

class conv5_relu(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(32),
            nn.ReLU(), 

            nn.Conv2d(32, 64, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(64),
            nn.ReLU(), 

            nn.Conv2d(64, 96, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(96),
            nn.ReLU(),

            nn.Conv2d(96, 128, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.Conv2d(128, 160, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(160),
            nn.ReLU(),

            nn.Flatten(1, -1),
            nn.Linear(10240, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#as in the paper except for GELU
class conv5(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 64, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(), 

            nn.Conv2d(64, 96, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(),

            nn.Conv2d(96, 128, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(128),
            nn.GELU(),

            nn.Conv2d(128, 160, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(160),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(10240, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#only first three layers from the paper to train faster
class conv5_(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 32, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(32),
            nn.GELU(), 

            nn.Conv2d(32, 64, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(64),
            nn.GELU(), 

            nn.Conv2d(64, 96, kernel_size = 5, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(24576, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

#as in the paper except for GELU
class conv7(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 48, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(48),
            nn.GELU(), 

            nn.Conv2d(48, 96, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(96),
            nn.GELU(), 

            nn.Conv2d(96, 144, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(144),
            nn.GELU(),

            nn.Conv2d(144, 192, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(192),
            nn.GELU(),

            nn.Flatten(1, -1),
            nn.Linear(3072, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

class conv7_relu(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # I am using gelu instead of relu, because Dan said in the video that this is better for adversarial robustness
            nn.Conv2d(1, 48, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(48),
            nn.ReLU(), 

            nn.Conv2d(48, 96, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(96),
            nn.ReLU(), 

            nn.Conv2d(96, 144, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(144),
            nn.ReLU(),

            nn.Conv2d(144, 192, kernel_size = 7, device=args['device']),
            nn.BatchNorm2d(192),
            nn.ReLU(),

            nn.Flatten(1, -1),
            nn.Linear(3072, 10, device=args['device']), 
            nn.BatchNorm1d(10),

            #nn.Softmax(dim = 1)
        )

    def forward(self, x):
        return(self.main(x))

"""## Train & test"""

def test(model, loader):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in loader:
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        with torch.no_grad():
          output = model(data)
          test_loss += F.cross_entropy(output, target, reduction = 'sum').data.item()
          pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
          correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()

    test_loss /= len(loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(val_loader.dataset),
        100. * correct / len(val_loader.dataset)))

from IPython.core.inputtransformer2 import TransformerManager
def train(
    train_loader,
    test_loader,
    model,
    num_epochs = 10, 
    batch_size = 120, 
    adversarial_attack=None,
    adversarial_lambda=0.0,
    outlier_loader = None,
    outlier_lambda = 0.5 #in Dan's paper it was 0.5 for vision tasks
  ):
  optimizer = optim.Adam(model.parameters(), lr=args['lr_conv'], weight_decay=args['weight_decay'])
  scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=args['gamma'])

  #outlier exposure with fashion MNIST dataset
  if outlier_loader:
    for i in range(1, num_epochs+1):
      model.train()
      for in_data, out_data in zip(train_loader, outlier_loader):
        data, target = in_data
        ood_data, _ = out_data
        if args['cuda']:
          data, target, ood_data = data.cuda(), target.cuda(), ood_data.cuda()
        data, target, ood_data = Variable(data), Variable(target), Variable(ood_data)
        optimizer.zero_grad()
        output = model(data)
        adoutput = model(adversarial_attack(data)) if adversarial_attack else None
        loss = F.cross_entropy(output, target)
        if adversarial_attack:
          loss += adversarial_lambda * F.cross_entropy(adoutput, target)
        ood_output = model(ood_data)
        loss += outlier_lambda * (-ood_output.mean(dim = 1)+torch.logsumexp(ood_output, dim = 1)).mean()
        loss.backward()
        optimizer.step()
        scheduler.step()
      print('epoch: ', i) 
      test(model, test_loader)     

  else:
    for i in range(1, num_epochs+1):
      model.train()
      for batch_idx, (data, target) in enumerate(train_loader):
        if args['cuda']:
          data, target = data.cuda(), target.cuda()
        #Variables in Pytorch are differenciable. 
        data, target = Variable(data), Variable(target)
        #This will zero out the gradients for this batch. 
        optimizer.zero_grad()
        output = model(data)
        adoutput = model(adversarial_attack(data, target)) if adversarial_attack else None
        #output = torch.log(output)
        loss = F.cross_entropy(output, target)
        if adversarial_attack:
          loss += adversarial_lambda * F.cross_entropy(adoutput, target)
        #dloss/dx for every Variable 
        loss.backward()
        #to do a one-step update on our parameter.
        optimizer.step()
        scheduler.step()
      print('epoch: ', i)
      test(model, test_loader)

model = conv3_() # I am using the model with less layers than the original paper
if args['cuda']:
    model.cuda()

#printing validation loss after first epoch
train(train_loader, val_loader, model, num_epochs = args['ensemble_epochs'])

class Ensemble(nn.Module):
    def __init__(self, models, temp):
        super().__init__()
        self.models = models
        self.temp = temp

    def forward(self, x):
        out = sum(model(x) for model in self.models)
        return out / self.temp

model1oe = conv3()
model2oe = conv5()
model3oe = conv7()
model4oe = conv3_()
model5oe = conv5_()

if args['cuda']:
    model1oe.cuda()
    model2oe.cuda()
    model3oe.cuda()
    model4oe.cuda()
    model5oe.cuda()

train(train_loader, val_loader, model1oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model2oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model3oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model4oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)
train(train_loader, val_loader, model5oe, num_epochs = args['ensemble_epochs'], outlier_loader = outlier_loader)

model1 = conv3()
model2 = conv5()
model3 = conv7()
model4 = conv3_()
model5 = conv5_()

if args['cuda']:
    model1.cuda()
    model2.cuda()
    model3.cuda()
    model4.cuda()
    model5.cuda()

train(train_loader, val_loader, model1, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model2, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model3, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model4, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model5, num_epochs = args['ensemble_epochs'])

model1r = conv3_relu()
model2r = conv5_relu()
model3r = conv7_relu()

if args['cuda']:
    model1r.cuda()
    model2r.cuda()
    model3r.cuda()

train(train_loader, val_loader, model1r, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model2r, num_epochs = args['ensemble_epochs'])
train(train_loader, val_loader, model3r, num_epochs = args['ensemble_epochs'])

ensemble_relu = Ensemble([model1r, model2r, model3r], 3)

test(ensemble_relu, val_loader)

ensemble_oe = Ensemble([model1oe, model2oe, model3oe], 3)

test(ensemble_oe, val_loader)

ensemble_5_oe = Ensemble([model1oe, model2oe, model3oe, model4oe, model5oe], 5)

test(ensemble_5_oe, val_loader)

ensemble = Ensemble([model1, model2, model3], 3)

test(ensemble, val_loader)

ensemble_5 = Ensemble([model1, model2, model3, model4, model5], 5)

test(ensemble_5, val_loader)

"""# OOD detection

"""

def get_auroc(_pos, _neg):
    y_pos = np.ones(np.shape(_pos))
    y_neg = np.zeros(np.shape(_neg))
    y = np.concatenate((y_pos, y_neg))
    y_pos = np.concatenate((_pos, _neg))
    auroc_score = sk.roc_auc_score(y, y_pos)
    return auroc_score

def max_logit_anomaly_score(output):
    score = torch.max(output, axis = 1)[0]
    score = -1*score
    return score.cpu()

#maybe numerically unstable - fix
def max_softmax_anomaly_score(output):
    score = torch.exp(output)
    score = score / torch.sum(score, axis = 1, keepdims = True)
    score = torch.max(score, axis = 1)[0]
    score = -1*score 
    return score.cpu() 

def cross_entropy_anomaly_score(output):
    N, C = output.shape
    prosjek = torch.sum(output, axis = 1) / C
    score = torch.exp(output)
    score = torch.sum(score, axis = 1)
    score = torch.log(score)
    score = prosjek - score
    return score.cpu()

def perturbate(model, data, epsilon, temp):
  '''
  data.requires_grad = True
  output = model(data)
  output.detach()
  ouput = output / temp
  s_y_log = F.softmax(output, dim=1).max(axis=1).values.log()
  s_y_log.backward()
  with torch.no_grad():
    grad = data.grad
    data_per = data-epsilon*torch.sign(-grad) 
  model.zero_grad()
  return data_per
  '''
  data.requires_grad = True
  output = model(data)
  output_n = output.detach().cpu().numpy()
  output_n = output_n / temp #typically ODIN requires high temperature, but our calibration temp is 1
  output_n = output_n - np.max(output_n)   # TODO: AK: Why do we subtract max? Normalize? 
  output_n = np.exp(output_n)
  output_n = output_n / np.sum(output_n)  
  label = np.argmax(output_n)
  label_ = torch.LongTensor([label]).cuda()
  label_ = Variable(label_)
  loss = F.cross_entropy(output/temp, label_)
  loss.backward()
  with torch.no_grad():
    grad = data.grad
    data_per = data-epsilon*torch.sign(grad)  # TODO: AK: Also I don't understand why not `-grad` as per the paper.
  model.zero_grad()
  return data_per

def auroc_score(ensemble, in_loader, ood_loader, method, odin = 0, epsilon = 0.002, temp = 10):
  in_score = []
  out_score = []

  def fill_score(myscore, myloader):
    for ind, (data, target) in enumerate(myloader):
      if (ind >= 200):
        break
      ubaci = data.clone()
      if args['cuda']:
        data, target, ubaci = data.cuda(), target.cuda(), ubaci.cuda()
      data, target, ubaci = Variable(data), Variable(target), Variable(ubaci)
      if odin == 1:
        for i in range (data.shape[0]):
          ubaci[i:i+1] = perturbate(ensemble, data[i:i+1], epsilon, temp)
      with torch.no_grad():
        output = ensemble(ubaci)
        if odin == 1:
          output = output / temp
        myscore.append(method(output).numpy()) 

  fill_score(in_score, in_loader)
  fill_score(out_score, ood_loader)

  """
  for ind, (data, target) in enumerate(in_loader):
    if (ind >= 200):
      break
    ubaci = data.clone()
    if args['cuda']:
      data, target, ubaci = data.cuda(), target.cuda(), ubaci.cuda()
    data, target, ubaci = Variable(data), Variable(target), Variable(ubaci)
    if odin == 1:
      for i in range (data.shape[0]):
        ubaci[i:i+1] = perturbate(ensemble, data[i:i+1], epsilon, temp)
    with torch.no_grad():
      output = ensemble(ubaci)
      if odin == 1:
        output = output / temp
      in_score.append(method(output).numpy()) 

  for ind, (data, target) in enumerate(ood_loader):
    if (ind >= 1000):
      break
    ubaci = data.clone()
    if args['cuda']:
      data, target, ubaci = data.cuda(), target.cuda(), ubaci.cuda()
    data, target, ubaci = Variable(data), Variable(target), Variable(ubaci)
    if odin == 1:
      for i in range (data.shape[0]):
        ubaci[i:i+1] = perturbate(ensemble, data[i:i+1], epsilon, temp)
    with torch.no_grad():
      output = ensemble(ubaci)
      if odin == 1:
        output = output / temp
      out_score.append(method(output).numpy()) 
  """

  in_score = np.concatenate(in_score)
  out_score = np.concatenate(out_score)

  return get_auroc(out_score, in_score)

print(auroc_score(ensemble, val_loader, ood_val_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3))

index =['AUROC max logit', 'AUROC softmax max', 'AUROC cross entropy', 'AUROC ODIN']
ood_report = pd.DataFrame(index=index)
def add_to_ood_report(col, calcs):
  ood_report[col] = pd.Series([calcs[k] for k in ('logit', 'softmax', 'cross', 'odin')], index=index)

calcs_ensemble_oe = {
    'logit' : auroc_score(ensemble_oe, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble_oe, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble_oe, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble_oe, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble OE', calcs_ensemble_oe)

calcs_ensemble = {
    'logit' : auroc_score(ensemble, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble', calcs_ensemble)

calcs_ensemble_5 = {
    'logit' : auroc_score(ensemble_5, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble_5, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble_5, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble_5, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble 5', calcs_ensemble_5)

calcs_ensemble_5_oe = {
    'logit' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, max_logit_anomaly_score),
    'softmax' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, max_softmax_anomaly_score),
    'cross' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, cross_entropy_anomaly_score),
    'odin' : auroc_score(ensemble_5_oe, test_loader, ood_test_loader, max_softmax_anomaly_score, odin = 1, epsilon = 0.1, temp = 1/3)
}

add_to_ood_report('Ensemble 5 OE', calcs_ensemble_5_oe)

ood_report

ood_report.to_latex()

"""# Adversarial Robustness

"""

#loading the data for the weak nn, make sure that you run the cell at the beginning that downloads mnist dataset
# load the data with validation set
adv_r_train_loader = torch.utils.data.DataLoader(
  train_subset,
  #transform=transforms.Normalize((0.1307,), (0.3081,)),
  batch_size = args['batch_size'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

adv_r_val_loader = torch.utils.data.DataLoader(
  val_subset,
  #transform=transforms.Normalize((0.1307,), (0.3081,)),
  batch_size = args['test_batch_size'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)

adv_r_test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('../data',
                   train=False,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args['test_batch_size'], shuffle=True, **kwargs)

class WeakNet(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super(WeakNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, device=args['device'])
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, device=args['device'])
        self.conv2_drop = nn.Dropout2d()  #Dropout
        self.fc1 = nn.Linear(320, 50, device=args['device'])
        self.fc2 = nn.Linear(50, 10, device=args['device'])

    def forward(self, x):
        #Convolutional Layer/Pooling Layer/Activation
        x = F.relu(F.max_pool2d(self.conv1(x), 2)) 
        #Convolutional Layer/Dropout/Pooling Layer/Activation
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        #Fully Connected Layer/Activation
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        #Fully Connected Layer/Activation
        x = self.fc2(x)
        #Softmax gets probabilities. 
        return F.log_softmax(x, dim=1)

def train_weaknn(epoch, model):
    model.train()
    for batch_idx, (data, target) in enumerate(adv_r_train_loader):
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        #Variables in Pytorch are differenciable. 
        data, target = Variable(data), Variable(target)
        #This will zero out the gradients for this batch. 
        optimizer.zero_grad()
        output = model(data)
        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.
        loss = F.nll_loss(output, target)
        #dloss/dx for every Variable 
        loss.backward()
        #to do a one-step update on our parameter.
        optimizer.step()
        #Print out the loss periodically. 
        if batch_idx % args['log_interval'] == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(adv_r_train_loader.dataset),
                100. * batch_idx / len(adv_r_train_loader), loss.item()))

def test_weaknn(model):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in adv_r_val_loader:
        if args['cuda']:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()

    test_loss /= len(adv_r_val_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(adv_r_val_loader.dataset),
        100. * correct / len(adv_r_val_loader.dataset)))

weakModel = WeakNet()
model = weakModel
if args['cuda']:
    weakModel.cuda()

optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])

for epoch in range(1, args['epochs'] + 5):
    train_weaknn(epoch, weakModel)
    test_weaknn(weakModel)

def adv_attack_nn(model, images, labels, verbose=False):
  def info(*args):
    if verbose:
      print(*args)
  predictions = model(images).argmax(axis=1).detach()
  results = []
  for Attack in (
      # L2
      fbattacks.L2BasicIterativeAttack,
      fbattacks.L2AdditiveGaussianNoiseAttack,
      fbattacks.L2DeepFoolAttack,
      fbattacks.L2FastGradientAttack,
      # Linf
      fbattacks.LinfFastGradientAttack,
      fbattacks.LinfPGD,
      # L0
      fbattacks.SaltAndPepperNoiseAttack,
  ):
    info('========', Attack.__name__, '=======\n')
    attack = Attack()
    epsilons = [0.3, 1.5, 10] #, 2.5, 5.0, 7.0]
    _, eps_clippedadv, eps_is_adv = attack(
        fmodel,
        images,
        labels,
        epsilons=epsilons
    )
    for eps, clippedadv, is_adv in zip(
        epsilons,
        eps_clippedadv, 
        eps_is_adv
      ):
      info(f' Epsilon: {eps}')
      robust_accuracy = 1 - is_adv.float().mean().item()
      info(f'  Robust accuracy {robust_accuracy*100:.2f}%')
      info('  Sample:')
      advindices = is_adv.long().nonzero().flatten().tolist()
      results += [[Attack.__name__, eps, robust_accuracy]]
      if advindices:
        indx = advindices[0]
    
        if verbose:
          plt.imshow(images[indx][0].cpu().numpy(), cmap='gray')
          plt.title('orig')
          plt.axis('off')
          plt.show()
          #fbplot.images(clippedadv[is_adv][:5])
          plt.imshow(clippedadv[indx][0].cpu().numpy(), cmap='gray')
          plt.title('adv')
          plt.axis('off')
          plt.show()
    
        info('   Prediction for usual:', predictions[indx].item())
        info('   Label:', labels[indx].item())
        advprediction = model(clippedadv[indx:indx+1]).detach().argmax(axis=1).item()
        info('   Prediction for adv.:', advprediction)
  return results

images, labels = next(iter(train_loader))
images = images.cuda()
labels = labels.cuda()

fmodel = FBModel(weakModel, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'CNN clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
cnn_res = adv_attack_nn(fmodel, images, labels)
index = [(e[0], e[1]) for e in cnn_res]
report = pd.DataFrame(index=index)
report['CNN'] = pd.Series([e[2] for e in cnn_res], index=index)
report

fmodel = FBModel(ensemble, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'Ensemble clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
ensemble_res = adv_attack_nn(fmodel, images, labels,)
report['Ensemble'] = pd.Series([e[2] for e in ensemble_res],index=index)
report

fmodel = FBModel(ensemble_5, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'Ensemble5 clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
ensemble_res = adv_attack_nn(fmodel, images, labels,)
report['Ensemble5'] = pd.Series([e[2] for e in ensemble_res],index=index)
report

fmodel = FBModel(ensemble_relu, bounds=(images.min().item(),images.max().item()), device='cuda')
print(f'EnsembleRELU clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
ensemble_res = adv_attack_nn(fmodel, images, labels,)
report['EnsembleRELU'] = pd.Series([e[2] for e in ensemble_res],index=index)
report

print('Median CNN, Ensemble: ', report['CNN'].median(), report['Ensemble'].median())
print('Median CNN, Ensemble5: ', report['CNN'].median(), report['Ensemble5'].median())
print('Mean:', (report['CNN'] - report['Ensemble']).mean())
print('Mean:', (report['CNN'] - report['Ensemble5']).mean())

def get_formatted(r):
  f = r.copy()
  f.index = [f"{e[0]}, ε={e[1]}" for e in f.index]
  f['CNN'] = [f"{e*100:.2f}%" for e in f['CNN']]
  f['Ensemble'] = [f"{e*100:.2f}%" for e in f['Ensemble']]
  f['Ensemble5'] = [f"{e*100:.2f}%" for e in f['Ensemble5']]
  return f
freport = get_formatted(report)
freport

freport.to_latex()

"""## Adversarial Training

"""

import copy 
report = pd.DataFrame(index=index)
adv_train_subset, adv_val_subset, _ = torch.utils.data.random_split(
  mnist_train,
  [int(mnist_train_n * 1/12), int(mnist_train_n*1/12), int(mnist_train_n*10/12)],  # Adv. example generation is slow. So take a few.
  generator=torch.Generator().manual_seed(1)
)

adv_train_loader =  torch.utils.data.DataLoader(
  adv_train_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
adv_val_loader =  torch.utils.data.DataLoader(
  adv_val_subset,
  batch_size = args['batch_size_conv'],
  shuffle = True,
  num_workers = args['num_workers'],
  **kwargs
)
epsilon = 1.5
at_epochs = 3
attack = fbattacks.LinfProjectedGradientDescentAttack()

for at_lambda in (0.2, 0.75):
  print(f"------\n Lambda={at_lambda} \n ------\n")

  for innerm in (model1, model2, model3): #, model4, model5):
    innermodelcopy = copy.deepcopy(innerm)
    mymodel = FBModel(innermodelcopy, bounds=(images.min().item(),images.max().item()), device='cuda')
    #mymodel = FBModel(innerm, bounds=(images.min().item(),images.max().item()), device='cuda')
    def adv_attack(images, labels):
      global mymodel
      _, clippedadv, _ = attack(
          mymodel,
          images,
          labels,
          epsilons=epsilon
      )
      return clippedadv
    train(
        adv_train_loader,
        adv_val_loader, 
        innerm, 
        num_epochs = at_epochs, 
        adversarial_attack=adv_attack, 
        adversarial_lambda=at_lambda
      )

  fmodel = FBModel(ensemble, bounds=(images.min().item(),images.max().item()), device='cuda')
  print(f'AT Ensemble clean accuracy: {fbutils.accuracy(fmodel, images, labels)}')
  ensemble_res = adv_attack_nn(fmodel, images, labels, verbose=True)
  report[f'AT Ensemble {at_lambda} {at_epochs}'] = pd.Series([e[2] for e in ensemble_res],index=index)
report



"""# Calibration"""

bins_ints = list(e/10 for e in range(0, 10, 1))
bins_ints

def calc_calibration(model):
  EPSILON = 0.0000001
  count = 0
  bins = {b : defaultdict(float) for b in bins_ints}
  for images, labels in train_loader:
    count += len(images)
    images = images.cuda()
    labels = labels.cuda()
    predictions = model(images).softmax(axis=1)
    for ps, l in zip(predictions, labels):
      maxp = ps.max(axis=0)
      plabel, prob  = maxp.indices.item(), maxp.values.item()
      mybin = bins[int(prob * 10) / 10.0]
      mybin['psum'] += prob
      mybin['msum'] += float(l == plabel)
      mybin['count'] += 1.0
  for k in bins:
    b = bins[k]
    b['rms_error'] = (b['psum'] - b['msum'])**2 / (b['count'] + EPSILON)/ (count + EPSILON)
    b['accuracy'] = b['msum'] / (b['count'] + EPSILON)
    b['confidence'] = b['psum'] / (b['count'] + EPSILON)
    b['ece'] = abs(b['accuracy'] - b['confidence']) * b['count'] / (count + EPSILON)
    b['mce'] = abs(b['accuracy'] - b['confidence'])
  rms_error = sum(bins[k]['rms_error'] for k in bins)**.5  # TODO: See below results. Why not changed?
  ece = sum(bins[k]['ece'] for k in bins)
  mce = max(bins[k]['mce'] for k in bins)
  calcs = {
      'rms_error': rms_error,
      'ece': ece,
      'mce': mce,
      'bins': bins,
      'count': count
  }
  return calcs

def print_calibration_report(calcs):
  bins = calcs['bins']
  print(f"RMS error: {calcs['rms_error']}")
  print(f"ECE: {calcs['ece']}")
  print(f"MCE: {calcs['mce']}")
  r = pd.DataFrame(index=bins_ints)
  r['score'] = pd.Series([bins[k]['res'] for k in bins_ints], index=bins_ints)
  r['count'] = pd.Series([bins[k]['count'] for k in bins_ints], index=bins_ints)
  r['msum'] = pd.Series([bins[k]['msum'] for k in bins_ints], index=bins_ints)
  r['psum'] = pd.Series([bins[k]['psum'] for k in bins_ints], index=bins_ints)
  r['accuracy'] = pd.Series([bins[k]['accuracy'] for k in bins_ints], index=bins_ints)
  r.loc['Total'] = r.sum(numeric_only=True)
  def accuracybar(r):
    r = r.copy()
    r = r.drop('Total')
    index = bins_ints
    r2 = pd.DataFrame({
        'accuracy': r['accuracy'],
        'bins': bins_ints
      },
      index=bins_ints
    )
    r2.plot.bar()
    plt.show()
  def density(r):
    r = r.copy()
    r = r.drop('Total')
    index = bins_ints
    r2 = pd.DataFrame({
        '% of samples': r['count']/calcs['count'],
        'bins': bins_ints
      },
      index=bins_ints
    )
    r2.plot.bar()
    plt.show()
  accuracybar(r)
  density(r)

index =['RMS', 'ECE', 'MCE']
calibration_report = pd.DataFrame(index=index)
def add_to_report(col, calcs):
  calibration_report[col] = pd.Series([calcs[k] for k in ('rms_error', 'ece', 'mce')], index=index)

calcs = calc_calibration(weakModel)
print_calibration_report(calcs)
add_to_report('CNN', calcs)

calcs = calc_calibration(ensemble)
print_calibration_report(calcs)
add_to_report('Vanilla Ensemble', calcs)

calcs = calc_calibration(ensemble5)
print_calibration_report(calcs)
add_to_report('Ensemble5', calcs)

temperatura = [0.7, 0.8, 0.9, 1, 2, 3]

for temp in temperatura:
  ensemble = Ensemble([model1, model2, model3], temp)
  calcs = calc_calibration(ensemble)
  print("Eto temperatura: ", temp)
  print_calibration_report(calcs)
  add_to_report(f'Ensemble. T={temp}', calcs)

temperatura = [3, 5, 10]

for temp in temperatura:
  ensemble = Ensemble([model1, model2, model3, model4, model5], temp)
  calcs = calc_calibration(ensemble)
  print("Eto temperatura: ", temp)
  print_calibration_report(calcs)
  add_to_report(f'Ens5. T={temp}', calcs)

calibration_report



"""# Distribution shift

"""

